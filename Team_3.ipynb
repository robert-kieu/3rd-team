{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "51jRU8qnRRNW",
        "w6TjupWzCZ1R"
      ],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "kJHUxPPiFbTQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# To ignore warinings\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "rhzkSx_JSfRl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1ehfGR5ZhmLU",
        "outputId": "300219b8-f537-45ce-d1c0-abf56d5fca9e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Random forest là gì ?"
      ],
      "metadata": {
        "id": "A5ZduScIKvgX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Định nghĩa."
      ],
      "metadata": {
        "id": "DL3eA5s2RMSs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Random forest là một phương pháp thống kê mô hình hóa bằng máy (machine learning statistic) dùng để phục vụ các mục đích phân loại, hồi quy và các nhiệm vụ khác bằng cách xây dựng nhiều cây quyết định (Decision tree).\n",
        "\n",
        "[_Nguồn_](https://jgac.vn/journal/article/view/344#:~:text=Random%20forest%20l%C3%A0%20m%E1%BB%99t%20ph%C6%B0%C6%A1ng,quy%E1%BA%BFt%20%C4%91%E1%BB%8Bnh%20(Decision%20tree).)\n",
        "\n",
        "Random Forest được ghi nhận hiệu quả hơn so với thuật toán phân loại khác thường được sử dụng vì có khả năng tìm ra thuộc tính nào quan trọng hơn so với những thuộc tính khác.\n",
        "\n",
        "</br>\n",
        "\n",
        "![](https://res.cloudinary.com/dyd911kmh/image/upload/v1677239993/image5_c214968fd6.png)\n",
        "\n",
        "<br>\n",
        "\n",
        "**Ưu điểm:**\n",
        "\n",
        "Random forests được coi là một phương pháp chính xác và mạnh mẽ vì số cây quyết định tham gia vào quá trình này. Thuật toán có thể được sử dụng trong cả hai vấn đề phân loại và hồi quy.\n",
        "\n",
        "<br>\n",
        "\n",
        "**Nhược điểm:**\n",
        "\n",
        "Random forests chậm tạo dự đoán bởi vì nó có nhiều cây quyết định. Bất cứ khi nào nó đưa ra dự đoán, tất cả các cây trong rừng phải đưa ra dự đoán cho cùng một đầu vào cho trước và sau đó thực hiện bỏ phiếu trên đó. Toàn bộ quá trình này tốn rất nhiều thời gian.\n",
        "\n",
        "Mô hình khó hiểu hơn so với cây quyết định, nơi ta có thể dễ dàng đưa ra quyết định bằng cách đi theo đường dẫn trong cây."
      ],
      "metadata": {
        "id": "vVF57YJxKnDP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cách thức hoạt động."
      ],
      "metadata": {
        "id": "eR7caJRLN6X4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Chọn các mẫu ngẫu nhiên từ tập dữ liệu đã cho.\n",
        "\n",
        "2. Thiết lập cây quyết định cho từng mẫu và nhận kết quả dự đoán từ mỗi quyết định cây.\n",
        "\n",
        "3. Bỏ phiếu cho mỗi kết quả dự đoán.\n",
        "\n",
        "4. Chọn kết quả được dự đoán nhiều nhất làm dự đoán cuối cùng.\n",
        "\n",
        "<br>\n",
        "\n",
        "![](https://images.viblo.asia/33adb558-67ba-4262-809e-3f8a8348e0c8.png)"
      ],
      "metadata": {
        "id": "uGUNsC3lN-MM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Decision tree."
      ],
      "metadata": {
        "id": "P236eSEqcLSG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Decision tree (cây quyết định) là một mô hình học máy dựa trên việc xây dựng cây có cấu trúc phân cấp để đưa ra các quyết định hoặc dự đoán. Cây quyết định được sử dụng chủ yếu trong bài toán phân loại (classification) và dự đoán (prediction).\n",
        "\n",
        "<br>\n",
        "\n",
        "Cây quyết định có cấu trúc giống cây với các nút (node) và các nhánh (branch). Các nút trong cây đại diện cho các quyết định hoặc các thuộc tính và các nhánh đại diện cho các kết quả hoặc các lựa chọn khả thi. Từ nút gốc, cây phân nhánh xuống các nút con dựa trên giá trị của thuộc tính được chọn để tối ưu hóa việc phân loại hoặc dự đoán. Quá trình này tiếp tục cho đến khi đạt được các nút lá (leaf node) đại diện cho các kết quả cuối cùng.\n",
        "\n",
        "<br>\n",
        "\n",
        "![](https://insidelearningmachines.com/wp-content/uploads/2021/02/tree_diagram.png)\n",
        "\n",
        "<br>\n",
        "\n",
        "Trong quá trình xây dựng cây quyết định, thuật toán sẽ sử dụng các tiêu chí như information gain (lợi ích thông tin), Gini index hoặc chi-square để đánh giá và lựa chọn thuộc tính phân nhánh tốt nhất. Các quyết định và dự đoán được thực hiện bằng cách đi xuống cây theo các nhánh tương ứng với các giá trị thuộc tính.\n",
        "\n",
        "<br>\n",
        "\n",
        "Trong bài toán này, team sẽ sử dụng information gain, cụ thể là overall entropy để tính toán cho nhanh vì `minimun overall entropy` = `maximun information gain`. Công thức tính như sau:\n",
        "\n",
        "<br>\n",
        "\n",
        "$ Overall Entropy = p_1 * Entropy(S_1) + p_2 * Entropy(S_2) + ... + p_n * Entropy(S_n)$\n",
        "\n",
        "Trong đó:\n",
        "\n",
        "- Overall Entropy là overall entropy.\n",
        "- p1, p2, ..., pn là tỷ lệ số lượng mẫu trong các tập dữ liệu con S1, S2, ..., Sn.\n",
        "- Entropy(S1), Entropy(S2), ..., Entropy(Sn) là entropy của các tập dữ liệu con.\n",
        "\n",
        "<br>\n",
        "\n",
        "Công thức tính entropy:\n",
        "\n",
        "$ Entropy(S) = - ∑ (p_i * log2(p_i)) $\n",
        "\n",
        "Trong đó:\n",
        "\n",
        "- Entropy(S) là entropy của tập dữ liệu S.\n",
        "- $p_i$ là tỷ lệ số lượng mẫu thuộc vào lớp i trong tập dữ liệu S.\n",
        "- log2 là hàm logarithm cơ số 2.\n",
        "\n"
      ],
      "metadata": {
        "id": "bB5vZdWAcO2F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ứng dụng thực tế."
      ],
      "metadata": {
        "id": "0W1PgE0pMqNd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Chủ yếu áp dụng vào việc giải các bài toán liên quan đến phân loại và hồi quy.\n",
        "\n",
        "<br>\n",
        "\n",
        "**Ví dụ:** Phân loại bệnh hoặc dự đoán giá cả hàng hóa trong tương lai."
      ],
      "metadata": {
        "id": "JmyiR-gVN5WA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hướng phát triển của nhóm."
      ],
      "metadata": {
        "id": "51jRU8qnRRNW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Áp dụng Numba, CUDA để chạy song song cho thuật toán Random forest nhằm mục đích cài thiện tốc độ trong quá trình sử dụng. Tiết kiệm thời gian huấn luyện và tận dụng tốt tài nguyên máy nhất có thể."
      ],
      "metadata": {
        "id": "IDh9aMAYRcva"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Nguồn tài liệu."
      ],
      "metadata": {
        "id": "w6TjupWzCZ1R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nhóm tìm được thuật toán Random Forest từ một [nguồn trên github](https://github.com/harrypnh/random-forest-from-scratch), họ implement thuật toán với 2 mục đích chính: chẩn đoán bệnh ung thư vú và đánh giá ô tô ( Car Evalution ).\n",
        "\n",
        "<br>\n",
        "\n",
        "**Tại sao lại chọn từ nguồn này?**\n",
        "\n",
        "Vì họ xây dựng thuật toán Random Forest theo hàm chứ không xây theo class, điều này khiến cho quá trình làm việc với numba trở nên dễ dàng hơn.\n"
      ],
      "metadata": {
        "id": "2tykp_EBC2EQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dữ liệu đầu vào."
      ],
      "metadata": {
        "id": "T4xfl4NsDDhG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Về phần dataset, có lẽ nhóm sẽ tìm một dataset nào đó đủ lớn và dễ hiểu để training model trước. Việc này sẽ dễ hơn là dùng lại dataset có sẵn vì các kiến thức về xe ô tô và y tế nhóm chưa có nhiều kinh nghiệm nên việc diễn giải sẽ khó hơn."
      ],
      "metadata": {
        "id": "EUw0852TDGmV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mục tiêu cần đạt được."
      ],
      "metadata": {
        "id": "TSRBREG_SoUc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "xây dựng ra tất cả 4 version:\n",
        "- V1 - là thuật toán Random Forest chạy tuần tự.\n",
        "- V2 - là thuật toán Random Forest chạy song song bằng CPU.\n",
        "- V3 - là thuật toán Random Forest chạy song song bằng GPU.\n",
        "- V4 - là thuật toán Random Forest chạy song song sử dụng shared memory.\n",
        "\n",
        "<br>\n",
        "\n",
        "Ở mức kỳ vọng 100% - nhóm mong muốn có thể xây dựng được thuật toán đến V3 ( GPU ) và tốc độ có sự cải thiện khi chạy với dataset đủ lớn.\n",
        "\n",
        "<br>\n",
        "\n",
        "Ở mức kỳ vọng 125% - nhóm mong muốn xây dựng được thuật toán đến V4 và có thể ứng dụng thuật toán của nhóm vào để thử giải quyết một bài toán thực tế nào đó do các thành viên nhóm đề cử ( ví dụ chuẩn đoán bệnh ).\n"
      ],
      "metadata": {
        "id": "Ly7so6GaSrcl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Utility functions"
      ],
      "metadata": {
        "id": "Lb2v_108BJ4l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Hàm hỗ trợ phân tách tập train - test\n",
        "\n",
        "def trainTestSplit(dataFrame, testSize):\n",
        "    if isinstance(testSize, float):\n",
        "        testSize = round(testSize * len(dataFrame))\n",
        "    indices = dataFrame.index.tolist()\n",
        "    testIndices = random.sample(population = indices, k = testSize)\n",
        "    dataFrameTest = dataFrame.loc[testIndices]\n",
        "    dataFrameTrain = dataFrame.drop(testIndices)\n",
        "    return dataFrameTrain, dataFrameTest"
      ],
      "metadata": {
        "id": "r7Yqcd5qBITP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Khái quát"
      ],
      "metadata": {
        "id": "XPcf3QDlG_mX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Để xây dựng thuật toán Random forest ( tuần tự ) thì ở đây, nhóm cần build 2 phần từ là:\n",
        "\n",
        "1. **DecisionTree:** Xử lý các vấn đề liên quan đến cây quyết định.\n",
        "\n",
        "2. **RandomForest:** Xử lý các vấn đề liên quan đến random forest.\n",
        "\n"
      ],
      "metadata": {
        "id": "luRJL1vFGn_G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Random Forest tuần tự **[ V1 ]**"
      ],
      "metadata": {
        "id": "ovLKUrPzF2fj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Decision Tree"
      ],
      "metadata": {
        "id": "dfCV_YOsEraH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy\n",
        "import pandas\n",
        "import random\n",
        "\n",
        "\n",
        "'''\n",
        "Kiểm tra data có thuộc một label duy nhất\n",
        "'''\n",
        "def checkPurity(data):\n",
        "    if len(np.unique(data[:, -1])) == 1:\n",
        "        return True\n",
        "    else:\n",
        "        return False\n",
        "\n",
        "\n",
        "'''\n",
        "Xác định label phổ biến nhất còn trong data và trả về label đó\n",
        "\n",
        "'''\n",
        "def classifyData(data):\n",
        "    uniqueClasses, uniqueClassesCounts = np.unique(data[:, -1], return_counts = True)\n",
        "    return uniqueClasses[uniqueClassesCounts.argmax()]\n",
        "\n",
        "\n",
        "'''\n",
        "Xác định cột (thuộc tính) và các giá trị split tiềm năng của cột đó\n",
        "'''\n",
        "def getPotentialSplits(data, randomAttributes):\n",
        "    potentialSplits = {}\n",
        "    _, columns = data.shape\n",
        "\n",
        "    # Lấy hết thuộc tính (trừ cột label cuối cùng) gán vào columnsIndices (List gồm index của các thuộc tính)\n",
        "    columnsIndices = list(range(columns - 1))\n",
        "\n",
        "    # Nếu có randomAttributes thì không lấy hết\n",
        "    if randomAttributes != None  and len(randomAttributes) <= len(columnsIndices):\n",
        "        columnsIndices = randomAttributes\n",
        "    for column in columnsIndices:\n",
        "        values = data[:, column]\n",
        "        # Xét độ unique của mỗi thuộc tính\n",
        "        uniqueValues = np.unique(values)\n",
        "\n",
        "        # Nếu thuộc tính chỉ có đúng 1 giá trị thì potentialSplits tại cột (thuộc tính) đó = chính giá trị đó\n",
        "        if len(uniqueValues) == 1:\n",
        "            potentialSplits[column] = uniqueValues\n",
        "\n",
        "        # còn không thì potentialSplits tại cột (thuộc tính) đó sẽ chứa các giá trị trung bình giữa 2 giá trị kế nhau trong mảng uniqueValues\n",
        "        else:\n",
        "            potentialSplits[column] = []\n",
        "            for i in range(len(uniqueValues)):\n",
        "                if i != 0:\n",
        "                    currentValue = uniqueValues[i]\n",
        "                    previousValue = uniqueValues[i - 1]\n",
        "                    potentialSplits[column].append((currentValue + previousValue) / 2)\n",
        "    return potentialSplits\n",
        "\n",
        "\n",
        "'''\n",
        "# Tách data thành 2 phần dựa vào splitValue\n",
        "'''\n",
        "def splitData(data, splitColumn, splitValue):\n",
        "    # Lấy data của cột thuộc tính đang đc xét để so sánh\n",
        "    splitColumnValues = data[:, splitColumn]\n",
        "\n",
        "    # Tách nguyên data (không phải chỉ mỗi data ở cột splitColumnValues) thành hai phần, một phần gồm các giá trị ở cột splitColumnValues <= splitValue và ngược lại\n",
        "    return data[splitColumnValues <= splitValue], data[splitColumnValues > splitValue]\n",
        "\n",
        "'''\n",
        "# Tính entropy của từng phần data\n",
        "'''\n",
        "def calculateEntropy(data):\n",
        "    # Tính entropy của data dựa vào tỷ lệ labels mà data thuộc về\n",
        "    _, uniqueClassesCounts = np.unique(data[:, -1], return_counts = True)\n",
        "    probabilities = uniqueClassesCounts / uniqueClassesCounts.sum()\n",
        "    return sum(probabilities * -np.log2(probabilities))\n",
        "\n",
        "\n",
        "'''\n",
        "# Tính tổng entropy\n",
        "'''\n",
        "def calculateOverallEntropy(dataBelow, dataAbove):\n",
        "    # Lấy tỷ lệ số lượng data ở nửa trên so với số lượng data\n",
        "    pDataBelow = len(dataBelow) / (len(dataBelow) + len(dataAbove))\n",
        "\n",
        "    # Lấy tỷ lệ số lượng data ở nửa dưới so với số lượng data\n",
        "    pDataAbove = len(dataAbove) / (len(dataBelow) + len(dataAbove))\n",
        "    return pDataBelow * calculateEntropy(dataBelow) + pDataAbove * calculateEntropy(dataAbove)\n",
        "\n",
        "\n",
        "'''\n",
        "Xác định cột (thuộc tính) và giá trị tiềm năng duy nhất của cột đó (splitPoint có entropy nhỏ nhất)\n",
        "'''\n",
        "def determineBestSplit(data, potentialSplits, randomSplits = None):\n",
        "    overallEntropy = 9999\n",
        "    bestSplitColumn = 0\n",
        "    bestSplitValue = 0\n",
        "    if randomSplits == None:\n",
        "        # Xét từng key là thuộc tính trong potentialSplits\n",
        "        for splitColumn in potentialSplits:\n",
        "            # Xét từng value của từng thuộc tính trong potentialSplits\n",
        "            for splitValue in potentialSplits[splitColumn]:\n",
        "                # Tách nguyên data của làm hai phần nhỏ hơn và lớn hơn so với value đang xét\n",
        "                dataBelow, dataAbove = splitData(data, splitColumn, splitValue)\n",
        "                # Xét Entropy của 2 phần data đó, nếu nhỏ nhất thì lấy\n",
        "                currentOverallEntropy = calculateOverallEntropy(dataBelow, dataAbove)\n",
        "                if currentOverallEntropy <= overallEntropy:\n",
        "                    overallEntropy = currentOverallEntropy\n",
        "                    bestSplitColumn = splitColumn\n",
        "                    bestSplitValue = splitValue\n",
        "    else:\n",
        "      # Tương tự như phần trên nhưng nó chọn random thuộc tính sau đó chọn random value để tính entropy\n",
        "        for i in range(randomSplits):\n",
        "            randomSplitColumn = random.choice(list(potentialSplits))\n",
        "            randomSplitValue = random.choice(potentialSplits[randomSplitColumn])\n",
        "            dataBelow, dataAbove = splitData(data, randomSplitColumn, randomSplitValue)\n",
        "            currentOverallEntropy = calculateOverallEntropy(dataBelow, dataAbove)\n",
        "            if currentOverallEntropy <= overallEntropy:\n",
        "                overallEntropy = currentOverallEntropy\n",
        "                bestSplitColumn = randomSplitColumn\n",
        "                bestSplitValue = randomSplitValue\n",
        "    return bestSplitColumn, bestSplitValue\n",
        "\n",
        "\n",
        "'''\n",
        "Xây dựng cây\n",
        "'''\n",
        "def buildDecisionTree(dataFrame, currentDepth = 0, minSampleSize = 2, maxDepth = 1000, randomAttributes = None, randomSplits = None):\n",
        "    # Nếu mới bắt đầu khởi tạo cây\n",
        "    if currentDepth == 0:\n",
        "        global COLUMN_HEADERS\n",
        "        COLUMN_HEADERS = dataFrame.columns\n",
        "        data = dataFrame.values\n",
        "\n",
        "        # Nếu randomAttributes = 0, xét hết tất cả thuộc tính còn không thì xét một vài thuộc tính được lấy random ra.\n",
        "        if randomAttributes != None and randomAttributes <= len(COLUMN_HEADERS) - 1:\n",
        "            randomAttributes = random.sample(population = list(range(len(COLUMN_HEADERS) - 1)), k = randomAttributes)\n",
        "        else:\n",
        "            randomAttributes = None\n",
        "    else:\n",
        "        data = dataFrame\n",
        "    # Xét nếu tất cả dữ liệu đều có label giống nhau || dữ liệu dữ liệu ít hơn số dữ liệu tối thiểu để xét cho một node || độ cao đã đạt tối đa\n",
        "    if checkPurity(data) or len(data) < minSampleSize or currentDepth == maxDepth:\n",
        "        # Trả về label cho node cuối cùng đó\n",
        "        return classifyData(data)\n",
        "\n",
        "    # Nếu không thì tiếp tục quá trình tạo cây\n",
        "    else:\n",
        "        currentDepth += 1\n",
        "        # Lấy ra potentialSplits là dictionaries gồm key là các thuộc tính, values là các giá trị potential của mỗi thuộc tính\n",
        "        potentialSplits = getPotentialSplits(data, randomAttributes)\n",
        "\n",
        "\n",
        "        # Lấy ra cột thuộc tính và giá trị split của thuộc tính mà mang lại giá trị entropy nhỏ nhất\n",
        "        splitColumn, splitValue = determineBestSplit(data, potentialSplits, randomSplits)\n",
        "\n",
        "        # split data theo 2 giá trị vừa lấy đc\n",
        "        dataBelow, dataAbove = splitData(data, splitColumn, splitValue)\n",
        "\n",
        "        # Nếu tất cả data đều thuộc một label rồi thì lấy label đó cho node\n",
        "        if len(dataBelow) == 0 or len(dataAbove) == 0:\n",
        "            return classifyData(data)\n",
        "        else:\n",
        "            # Đặt giá trị splitPoint cho thuộc tính đó, phần data nhỏ hơn sẽ qua yes và ngược lại\n",
        "            question = str(COLUMN_HEADERS[splitColumn]) + \" <= \" + str(splitValue)\n",
        "            decisionSubTree = {question: []}\n",
        "            # Tiếp tục rẽ nhánh bằng đệ quy\n",
        "            yesAnswer = buildDecisionTree(dataBelow, currentDepth, minSampleSize, maxDepth, randomAttributes, randomSplits)\n",
        "            noAnswer = buildDecisionTree(dataAbove, currentDepth, minSampleSize, maxDepth, randomAttributes, randomSplits)\n",
        "            # Nếu 2 nhánh giống nhau thì chỉ rẽ 1 nhánh, khác nhau thì rẽ 2 nhánh\n",
        "            if yesAnswer == noAnswer:\n",
        "                decisionSubTree = yesAnswer\n",
        "            else:\n",
        "                decisionSubTree[question].append(yesAnswer)\n",
        "                decisionSubTree[question].append(noAnswer)\n",
        "            return decisionSubTree\n",
        "\n",
        "\n",
        "'''\n",
        "Dùng để xét label cho tập dữ liệu bằng cách đưa mỗi cây trong randomForest\n",
        "'''\n",
        "def classifySample(sample, decisionTree):\n",
        "    # Nếu xét đến label của node lá cuối cùng thì trả về label đó\n",
        "    if not isinstance(decisionTree, dict):\n",
        "        return decisionTree\n",
        "    question = list(decisionTree.keys())[0]\n",
        "    attribute, value = question.split(\" <= \")\n",
        "    # So sánh với splitValue để mò đường đi tới node chứa label quyết định\n",
        "    if sample[attribute] <= float(value):\n",
        "        answer = decisionTree[question][0]\n",
        "    else:\n",
        "        answer = decisionTree[question][1]\n",
        "    return classifySample(sample, answer)\n",
        "\n",
        "\n",
        "'''\n",
        "Trả về label được dự đoán của tập data\n",
        "'''\n",
        "def decisionTreePredictions(dataFrame, decisionTree):\n",
        "    # Dự đoán từng label của từng dòng dữ liệu\n",
        "    predictions = dataFrame.apply(classifySample, axis = 1, args = (decisionTree,))\n",
        "    return predictions\n",
        "\n",
        "\n",
        "'''\n",
        "# So sánh độ chính xác giữa tập đang ktra và tập giá trị thật\n",
        "'''\n",
        "def calculateAccuracy(predictedResults, category):\n",
        "    resultCorrect = predictedResults == category\n",
        "    return resultCorrect.mean()"
      ],
      "metadata": {
        "id": "7Xc41kdFEk1H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Random forest"
      ],
      "metadata": {
        "id": "_FgwIc7ZEvHT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Dùng kỹ thuật boostrap để lấy random một subset của data\n",
        "'''\n",
        "def bootstrapSample(dataFrame, bootstrapSize):\n",
        "    randomIndices = np.random.randint(low = 0, high = len(dataFrame), size = bootstrapSize)\n",
        "    return dataFrame.iloc[randomIndices]\n",
        "\n",
        "\n",
        "'''\n",
        "Xây dựng rừng cây từ các cây decision tree\n",
        "'''\n",
        "def createRandomForest(dataFrame, bootstrapSize, randomAttributes, randomSplits, forestSize = 20, treeMaxDepth = 1000):\n",
        "    forest = []\n",
        "    for i in range(forestSize):\n",
        "        # Lấy random một subset của data bằng kỹ thuật boostrap\n",
        "        bootstrappedDataFrame = bootstrapSample(dataFrame, bootstrapSize)\n",
        "        # Xây dựng 1 cây quyết định từ subset đó\n",
        "        decisionTree = buildDecisionTree(bootstrappedDataFrame, maxDepth = treeMaxDepth, randomAttributes = randomAttributes, randomSplits = randomSplits)\n",
        "        # Thêm cây vào rừng\n",
        "        forest.append(decisionTree)\n",
        "    return forest\n",
        "\n",
        "\n",
        "'''\n",
        "Dự đoán data bằng cách đưa qua từng cây trong randomForest rồi lấy label theo số đông\n",
        "'''\n",
        "def randomForestPredictions(dataFrame, randomForest):\n",
        "    predictions = {}\n",
        "    # Đưa dữ liệu qua rừng cây (cụ thể là từng cây)\n",
        "    for i in range(len(randomForest)):\n",
        "\n",
        "        column = \"decision tree \" + str(i)\n",
        "        predictions[column] = decisionTreePredictions(dataFrame, randomForest[i])\n",
        "    predictions = pd.DataFrame(predictions)\n",
        "    # Lấy label có số lần xuất hiện nhiều nhất của từng dòng\n",
        "    return predictions.mode(axis = 1)[0]\n",
        "\n",
        "'''\n",
        "# So sánh độ chính xác giữa tập đang ktra và tập giá trị thật\n",
        "'''\n",
        "def calculateAccuracy(predictedResults, category):\n",
        "    # So sánh label của tập dự đoán và tập real\n",
        "    resultCorrect = predictedResults == category\n",
        "    return resultCorrect.mean()"
      ],
      "metadata": {
        "id": "uuULCgMAEwsI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sample run"
      ],
      "metadata": {
        "id": "T9VWgE7PE4En"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the dataset\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/Bài Tập Về Nhà Môn Cuối/iris.csv\")\n",
        "\n",
        "# label encoder\n",
        "from sklearn import preprocessing\n",
        "\n",
        "label_encoder = preprocessing.LabelEncoder()\n",
        "\n",
        "# Encode labels in column 'species'.\n",
        "df['species']= label_encoder.fit_transform(df['species'])\n",
        "\n",
        "%time\n",
        "randomForest = createRandomForest(df, bootstrapSize = 10, randomAttributes = 10, randomSplits = 50, numTree = 30, maxDepth = 3)\n",
        "randomForestTestResults = randomForestPredictions(df, randomForest)"
      ],
      "metadata": {
        "id": "zPLJtwMNiTzA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4f3ae770-356a-47b1-d559-6389d1a37535"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
            "Wall time: 4.53 µs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy = calculateAccuracy(randomForestTestResults, df.iloc[:, -1]) * 100"
      ],
      "metadata": {
        "id": "NVjBaZgujPQR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ygls9Kw4i468",
        "outputId": "4c12d9f3-5dc1-4190-fe90-bd2688614803"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "96.0"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Song song hóa sử dụng CPU **[ V2 ]**"
      ],
      "metadata": {
        "id": "4mSWkbQ_AFof"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Decision Tree V2.0"
      ],
      "metadata": {
        "id": "3q2yu7OkAMbY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy\n",
        "import pandas\n",
        "import random\n",
        "from numba import njit, prange\n",
        "\n",
        "\n",
        "\n",
        "def checkPurity(data):\n",
        "    if len(numpy.unique(data[:, -1])) == 1:\n",
        "        return True\n",
        "    else:\n",
        "        return False\n",
        "\n",
        "\n",
        "\n",
        "def classifyData(data):\n",
        "    uniqueClasses, uniqueClassesCounts = numpy.unique(data[:, -1], return_counts = True)\n",
        "    return uniqueClasses[uniqueClassesCounts.argmax()]\n",
        "\n",
        "\n",
        "\n",
        "def getPotentialSplits(data, randomAttributes):\n",
        "    potentialSplits = {}\n",
        "    _, columns = data.shape\n",
        "    columnsIndices = list(range(columns - 1))\n",
        "    if randomAttributes != None  and len(randomAttributes) <= len(columnsIndices):\n",
        "        columnsIndices = randomAttributes\n",
        "    for column in columnsIndices:\n",
        "        values = data[:, column]\n",
        "        uniqueValues = numpy.unique(values)\n",
        "        if len(uniqueValues) == 1:\n",
        "            potentialSplits[column] = uniqueValues\n",
        "        else:\n",
        "            potentialSplits[column] = []\n",
        "            for i in prange(len(uniqueValues)): # // ở đây bằng prange\n",
        "                if i != 0:\n",
        "                    currentValue = uniqueValues[i]\n",
        "                    previousValue = uniqueValues[i - 1]\n",
        "                    potentialSplits[column].append((currentValue + previousValue) / 2)\n",
        "    return potentialSplits\n",
        "\n",
        "\n",
        "\n",
        "def splitData(data, splitColumn, splitValue):\n",
        "    splitColumnValues = data[:, splitColumn]\n",
        "    return data[splitColumnValues <= splitValue], data[splitColumnValues > splitValue]\n",
        "\n",
        "\n",
        "njit(parallel=True)\n",
        "def calculateEntropy(data):\n",
        "    _, uniqueClassesCounts = numpy.unique(data[:, -1], return_counts = True)\n",
        "    probabilities = uniqueClassesCounts / uniqueClassesCounts.sum()\n",
        "\n",
        "    # // ở đây, thay thế hàm sum có sẵn trong python bằng vòng lặp sử dụng prange\n",
        "    entropy = 0.0\n",
        "    for i in prange(len(probabilities)):\n",
        "        entropy -= probabilities[i] * np.log2(probabilities[i])\n",
        "    return entropy\n",
        "\n",
        "\n",
        "\n",
        "def calculateOverallEntropy(dataBelow, dataAbove):\n",
        "    pDataBelow = len(dataBelow) / (len(dataBelow) + len(dataAbove))\n",
        "    pDataAbove = len(dataAbove) / (len(dataBelow) + len(dataAbove))\n",
        "    return pDataBelow * calculateEntropy(dataBelow) + pDataAbove * calculateEntropy(dataAbove)\n",
        "\n",
        "\n",
        "\n",
        "def determineBestSplit(data, potentialSplits, randomSplits = None):\n",
        "    overallEntropy = 9999\n",
        "    bestSplitColumn = 0\n",
        "    bestSplitValue = 0\n",
        "    if randomSplits == None:\n",
        "        for splitColumn in potentialSplits:\n",
        "            for splitValue in potentialSplits[splitColumn]:\n",
        "                dataBelow, dataAbove = splitData(data, splitColumn, splitValue)\n",
        "                currentOverallEntropy = calculateOverallEntropy(dataBelow, dataAbove)\n",
        "                if currentOverallEntropy <= overallEntropy:\n",
        "                    overallEntropy = currentOverallEntropy\n",
        "                    bestSplitColumn = splitColumn\n",
        "                    bestSplitValue = splitValue\n",
        "    else:\n",
        "        for i in prange(randomSplits): # // vòng for này bằng prange\n",
        "            randomSplitColumn = random.choice(list(potentialSplits))\n",
        "            randomSplitValue = random.choice(potentialSplits[randomSplitColumn])\n",
        "            dataBelow, dataAbove = splitData(data, randomSplitColumn, randomSplitValue)\n",
        "            currentOverallEntropy = calculateOverallEntropy(dataBelow, dataAbove)\n",
        "            if currentOverallEntropy <= overallEntropy:\n",
        "                overallEntropy = currentOverallEntropy\n",
        "                bestSplitColumn = randomSplitColumn\n",
        "                bestSplitValue = randomSplitValue\n",
        "    return bestSplitColumn, bestSplitValue\n",
        "\n",
        "\n",
        "\n",
        "def buildDecisionTree(dataFrame, currentDepth = 0, minSampleSize = 2, maxDepth = 1000, randomAttributes = None, randomSplits = None):\n",
        "    if currentDepth == 0:\n",
        "        global COLUMN_HEADERS\n",
        "        COLUMN_HEADERS = dataFrame.columns\n",
        "        data = dataFrame.values\n",
        "        if randomAttributes != None and randomAttributes <= len(COLUMN_HEADERS) - 1:\n",
        "            randomAttributes = random.sample(population = list(range(len(COLUMN_HEADERS) - 1)), k = randomAttributes)\n",
        "        else:\n",
        "            randomAttributes = None\n",
        "    else:\n",
        "        data = dataFrame\n",
        "    if checkPurity(data) or len(data) < minSampleSize or currentDepth == maxDepth:\n",
        "        return classifyData(data)\n",
        "    else:\n",
        "        currentDepth += 1\n",
        "        potentialSplits = getPotentialSplits(data, randomAttributes)\n",
        "        splitColumn, splitValue = determineBestSplit(data, potentialSplits, randomSplits)\n",
        "        dataBelow, dataAbove = splitData(data, splitColumn, splitValue)\n",
        "        if len(dataBelow) == 0 or len(dataAbove) == 0:\n",
        "            return classifyData(data)\n",
        "        else:\n",
        "            question = str(COLUMN_HEADERS[splitColumn]) + \" <= \" + str(splitValue)\n",
        "            decisionSubTree = {question: []}\n",
        "            yesAnswer = buildDecisionTree(dataBelow, currentDepth, minSampleSize, maxDepth, randomAttributes, randomSplits)\n",
        "            noAnswer = buildDecisionTree(dataAbove, currentDepth, minSampleSize, maxDepth, randomAttributes, randomSplits)\n",
        "            if yesAnswer == noAnswer:\n",
        "                decisionSubTree = yesAnswer\n",
        "            else:\n",
        "                decisionSubTree[question].append(yesAnswer)\n",
        "                decisionSubTree[question].append(noAnswer)\n",
        "            return decisionSubTree\n",
        "\n",
        "\n",
        "\n",
        "def classifySample(sample, decisionTree):\n",
        "    if not isinstance(decisionTree, dict):\n",
        "        return decisionTree\n",
        "    question = list(decisionTree.keys())[0]\n",
        "    attribute, value = question.split(\" <= \")\n",
        "    if sample[attribute] <= float(value):\n",
        "        answer = decisionTree[question][0]\n",
        "    else:\n",
        "        answer = decisionTree[question][1]\n",
        "    return classifySample(sample, answer)\n",
        "\n",
        "\n",
        "\n",
        "def decisionTreePredictions(dataFrame, decisionTree):\n",
        "    predictions = dataFrame.apply(classifySample, axis = 1, args = (decisionTree,))\n",
        "    return predictions\n",
        "\n",
        "\n",
        "\n",
        "def calculateAccuracy(predictedResults, category):\n",
        "    resultCorrect = predictedResults == category\n",
        "    return resultCorrect.mean()"
      ],
      "metadata": {
        "id": "-7OZFpHQAMbc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Random Forest V2.0"
      ],
      "metadata": {
        "id": "t8DVIv2WdJwg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def trainTestSplit(dataFrame, testSize):\n",
        "    if isinstance(testSize, float):\n",
        "        testSize = round(testSize * len(dataFrame))\n",
        "    indices = dataFrame.index.tolist()\n",
        "    testIndices = random.sample(population = indices, k = testSize)\n",
        "    dataFrameTest = dataFrame.loc[testIndices]\n",
        "    dataFrameTrain = dataFrame.drop(testIndices)\n",
        "    return dataFrameTrain, dataFrameTest\n",
        "\n",
        "def bootstrapSample(dataFrame, bootstrapSize):\n",
        "    randomIndices = numpy.random.randint(low = 0, high = len(dataFrame), size = bootstrapSize)\n",
        "    return dataFrame.iloc[randomIndices]\n",
        "\n",
        "\n",
        "\n",
        "def createRandomForest(dataFrame, bootstrapSize, randomAttributes, randomSplits, forestSize = 20, treeMaxDepth = 1000):\n",
        "    forest = []\n",
        "    for i in prange(forestSize): # // hóa với prange thay range\n",
        "        bootstrappedDataFrame = bootstrapSample(dataFrame, bootstrapSize)\n",
        "        decisionTree = buildDecisionTree(bootstrappedDataFrame, maxDepth = treeMaxDepth, randomAttributes = randomAttributes, randomSplits = randomSplits)\n",
        "        forest.append(decisionTree)\n",
        "    return forest\n",
        "\n",
        "\n",
        "\n",
        "def randomForestPredictions(dataFrame, randomForest):\n",
        "    predictions = {}\n",
        "    for i in prange(len(randomForest)): # // hóa với prange thay range\n",
        "        column = \"decision tree \" + str(i)\n",
        "        predictions[column] = decisionTreePredictions(dataFrame, randomForest[i])\n",
        "    predictions = pandas.DataFrame(predictions)\n",
        "    return predictions.mode(axis = 1)[0]\n",
        "\n",
        "def calculateAccuracy(predictedResults, category):\n",
        "    resultCorrect = predictedResults == category\n",
        "    return resultCorrect.mean()"
      ],
      "metadata": {
        "id": "RXLvnU-RdRQZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sample run"
      ],
      "metadata": {
        "id": "aRD5t9c6dVez"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the dataset\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/Bài Tập Về Nhà Môn Cuối/iris.csv\")\n",
        "\n",
        "%time\n",
        "randomForest = createRandomForest(df, bootstrapSize = 10, randomAttributes = 10, randomSplits = 50, forestSize = 30, treeMaxDepth = 3)\n",
        "randomForestTestResults = randomForestPredictions(df, randomForest)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2vit1jh6dXlO",
        "outputId": "fa09a2f4-2ba1-4f0d-8563-06176a7c1954"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 3 µs, sys: 0 ns, total: 3 µs\n",
            "Wall time: 7.39 µs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy = calculateAccuracy(randomForestTestResults, df.iloc[:, -1]) * 100\n",
        "accuracy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QAgEou26daOU",
        "outputId": "7668df26-a70f-4795-fb60-9edee64f5e7f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "95.33333333333334"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Song song hóa sử dụng CPU & GPU **[ V3 ]**"
      ],
      "metadata": {
        "id": "6-vhmeHofuiP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Decision Tree V3.0"
      ],
      "metadata": {
        "id": "_MflX4LEfuit"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy\n",
        "import pandas\n",
        "import random\n",
        "from numba import njit, prange, cuda, float32, int32\n",
        "import numba\n",
        "\n",
        "\n",
        "\n",
        "'''\n",
        "\n",
        "Hàm bên dưới xác định \"độ tinh khiết (Pure)\" của tập dữ liệu bằng cách kiểm tra xem nó\n",
        "\n",
        "có chứa cùng một label hay không, nếu có thì return true, ngược lại return false.\n",
        "\n",
        "'''\n",
        "\n",
        "def checkPurity(data):\n",
        "    if len(numpy.unique(data[:, -1])) == 1:\n",
        "        return True\n",
        "    else:\n",
        "        return False\n",
        "\n",
        "    # ...\n",
        "\n",
        "\n",
        "\n",
        "def classifyData(data):\n",
        "    uniqueClasses, uniqueClassesCounts = numpy.unique(data[:, -1], return_counts = True)\n",
        "    return uniqueClasses[uniqueClassesCounts.argmax()]\n",
        "\n",
        "    # ...\n",
        "\n",
        "\n",
        "\n",
        "def getPotentialSplits(data, randomAttributes):\n",
        "    potentialSplits = {}\n",
        "    _, columns = data.shape\n",
        "    columnsIndices = list(range(columns - 1))\n",
        "    if randomAttributes != None  and len(randomAttributes) <= len(columnsIndices):\n",
        "        columnsIndices = randomAttributes\n",
        "    for column in columnsIndices:\n",
        "        values = data[:, column]\n",
        "        uniqueValues = numpy.unique(values)\n",
        "        if len(uniqueValues) == 1:\n",
        "            potentialSplits[column] = uniqueValues\n",
        "        else:\n",
        "            potentialSplits[column] = []\n",
        "            for i in prange(len(uniqueValues)): # // ở đây bằng prange\n",
        "                if i != 0:\n",
        "                    currentValue = uniqueValues[i]\n",
        "                    previousValue = uniqueValues[i - 1]\n",
        "                    potentialSplits[column].append((currentValue + previousValue) / 2) # tại sao dừng công thức này ?\n",
        "    return potentialSplits\n",
        "\n",
        "\n",
        "\n",
        "def splitData(data, splitColumn, splitValue):\n",
        "    splitColumnValues = data[:, splitColumn]\n",
        "    return data[splitColumnValues <= splitValue], data[splitColumnValues > splitValue]\n",
        "\n",
        "\n",
        "# CUDA kernel ref. https://nyu-cds.github.io/python-numba/05-cuda/\n",
        "@cuda.jit\n",
        "def calculateEntropy(uniqueClassesCounts, lendata, result):\n",
        "\n",
        "    idx = cuda.grid(1)  # Get the thread index\n",
        "\n",
        "\n",
        "    if idx < lendata:\n",
        "      # t, uniqueClassesCounts = np.unique(data[:, -1], return_counts = True)\n",
        "      sum_uniqueClassesCounts = 0.0\n",
        "\n",
        "      for i in uniqueClassesCounts:\n",
        "        sum_uniqueClassesCounts += i\n",
        "\n",
        "      # probabilities = uniqueClassesCounts / sum_uniqueClassesCounts\n",
        "\n",
        "\n",
        "      # // ở đây, thay thế hàm sum có sẵn trong python bằng vòng lặp sử dụng prange\n",
        "      # entropy = 0.0\n",
        "      # for i in probabilities:\n",
        "      #     entropy -= i * cuda.log2(i)\n",
        "\n",
        "    result[idx] = 0.0\n",
        "\n",
        "    # ...\n",
        "\n",
        "\n",
        "\n",
        "def calculateOverallEntropy(dataBelow, dataAbove):\n",
        "\n",
        "    pDataBelow = len(dataBelow) / (len(dataBelow) + len(dataAbove))\n",
        "    pDataAbove = len(dataAbove) / (len(dataBelow) + len(dataAbove))\n",
        "\n",
        "    # Allocate device memory for entropy calculation\n",
        "    entropyBelow = cuda.to_device(len(dataBelow))\n",
        "    entropyAbove = cuda.to_device(len(dataAbove))\n",
        "\n",
        "    # calculate amout of unique class\n",
        "    _, Below_uniqueClassesCounts = np.unique(dataBelow[:, -1], return_counts = True)\n",
        "    _, Above_uniqueClassesCounts = np.unique(dataAbove[:, -1], return_counts = True)\n",
        "\n",
        "\n",
        "\n",
        "    # Launch CUDA kernels for entropy calculation\n",
        "    blockSize = 128\n",
        "    gridSizeBelow = (len(dataBelow) + blockSize - 1) // blockSize\n",
        "    gridSizeAbove = (len(dataAbove) + blockSize - 1) // blockSize\n",
        "\n",
        "    print(f'{dataBelow=}')\n",
        "\n",
        "\n",
        "    calculateEntropy[gridSizeBelow, blockSize](Below_uniqueClassesCounts, len(dataBelow), entropyBelow)\n",
        "    calculateEntropy[gridSizeAbove, blockSize](Above_uniqueClassesCounts, len(dataAbove), entropyAbove)\n",
        "\n",
        "    # Transfer results back to the host\n",
        "    entropyBelow = entropyBelow.copy_to_host()\n",
        "    entropyAbove = entropyAbove.copy_to_host()\n",
        "\n",
        "    print(entropyBelow, '\\n')\n",
        "    print(entropyAbove)\n",
        "\n",
        "    overallEntropy = pDataBelow * entropyBelow.sum() + pDataAbove * entropyAbove.sum()\n",
        "    return overallEntropy\n",
        "\n",
        "    # ...\n",
        "\n",
        "\n",
        "def determineBestSplit(data, potentialSplits, randomSplits = None):\n",
        "    overallEntropy = 9999\n",
        "    bestSplitColumn = 0\n",
        "    bestSplitValue = 0\n",
        "    if randomSplits == None:\n",
        "        for splitColumn in potentialSplits:\n",
        "            for splitValue in potentialSplits[splitColumn]:\n",
        "                dataBelow, dataAbove = splitData(data, splitColumn, splitValue)\n",
        "                currentOverallEntropy = calculateOverallEntropy(dataBelow, dataAbove)\n",
        "                if currentOverallEntropy <= overallEntropy:\n",
        "                    overallEntropy = currentOverallEntropy\n",
        "                    bestSplitColumn = splitColumn\n",
        "                    bestSplitValue = splitValue\n",
        "    else:\n",
        "        for i in prange(randomSplits): # // vòng for này bằng prange\n",
        "            randomSplitColumn = random.choice(list(potentialSplits))\n",
        "            randomSplitValue = random.choice(potentialSplits[randomSplitColumn])\n",
        "            dataBelow, dataAbove = splitData(data, randomSplitColumn, randomSplitValue)\n",
        "\n",
        "            # print(f'{randomSplitValue=}\\n{dataBelow=}\\n {dataAbove=}\\n\\n')\n",
        "            currentOverallEntropy = calculateOverallEntropy(dataBelow, dataAbove)\n",
        "            if currentOverallEntropy <= overallEntropy:\n",
        "                overallEntropy = currentOverallEntropy\n",
        "                bestSplitColumn = randomSplitColumn\n",
        "                bestSplitValue = randomSplitValue\n",
        "    return bestSplitColumn, bestSplitValue\n",
        "\n",
        "\n",
        "\n",
        "def buildDecisionTree(dataFrame, currentDepth = 0, minSampleSize = 2, maxDepth = 1000, randomAttributes = None, randomSplits = None):\n",
        "    if currentDepth == 0:\n",
        "        global COLUMN_HEADERS\n",
        "        COLUMN_HEADERS = dataFrame.columns\n",
        "        data = dataFrame.values\n",
        "        if randomAttributes != None and randomAttributes <= len(COLUMN_HEADERS) - 1:\n",
        "            randomAttributes = random.sample(population = list(range(len(COLUMN_HEADERS) - 1)), k = randomAttributes)\n",
        "        else:\n",
        "            randomAttributes = None\n",
        "    else:\n",
        "        data = dataFrame\n",
        "    if checkPurity(data) or len(data) < minSampleSize or currentDepth == maxDepth:\n",
        "        return classifyData(data)\n",
        "    else:\n",
        "        currentDepth += 1\n",
        "        potentialSplits = getPotentialSplits(data, randomAttributes)\n",
        "        splitColumn, splitValue = determineBestSplit(data, potentialSplits, randomSplits)\n",
        "        dataBelow, dataAbove = splitData(data, splitColumn, splitValue)\n",
        "        if len(dataBelow) == 0 or len(dataAbove) == 0:\n",
        "            return classifyData(data)\n",
        "        else:\n",
        "            question = str(COLUMN_HEADERS[splitColumn]) + \" <= \" + str(splitValue)\n",
        "            decisionSubTree = {question: []}\n",
        "            yesAnswer = buildDecisionTree(dataBelow, currentDepth, minSampleSize, maxDepth, randomAttributes, randomSplits)\n",
        "            noAnswer = buildDecisionTree(dataAbove, currentDepth, minSampleSize, maxDepth, randomAttributes, randomSplits)\n",
        "            if yesAnswer == noAnswer:\n",
        "                decisionSubTree = yesAnswer\n",
        "            else:\n",
        "                decisionSubTree[question].append(yesAnswer)\n",
        "                decisionSubTree[question].append(noAnswer)\n",
        "            return decisionSubTree\n",
        "\n",
        "\n",
        "\n",
        "def classifySample(sample, decisionTree):\n",
        "    if not isinstance(decisionTree, dict):\n",
        "        return decisionTree\n",
        "    question = list(decisionTree.keys())[0]\n",
        "    attribute, value = question.split(\" <= \")\n",
        "    if sample[attribute] <= float(value):\n",
        "        answer = decisionTree[question][0]\n",
        "    else:\n",
        "        answer = decisionTree[question][1]\n",
        "    return classifySample(sample, answer)\n",
        "\n",
        "\n",
        "\n",
        "def decisionTreePredictions(dataFrame, decisionTree):\n",
        "    predictions = dataFrame.apply(classifySample, axis = 1, args = (decisionTree,))\n",
        "    return predictions\n",
        "\n",
        "\n",
        "\n",
        "def calculateAccuracy(predictedResults, category):\n",
        "    resultCorrect = predictedResults == category\n",
        "    return resultCorrect.mean()"
      ],
      "metadata": {
        "id": "i4TBxO7Hfuit"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Random Forest V3.0"
      ],
      "metadata": {
        "id": "x7afKUaXfuiv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def trainTestSplit(dataFrame, testSize):\n",
        "    if isinstance(testSize, float):\n",
        "        testSize = round(testSize * len(dataFrame))\n",
        "    indices = dataFrame.index.tolist()\n",
        "    testIndices = random.sample(population = indices, k = testSize)\n",
        "    dataFrameTest = dataFrame.loc[testIndices]\n",
        "    dataFrameTrain = dataFrame.drop(testIndices)\n",
        "    return dataFrameTrain, dataFrameTest\n",
        "\n",
        "def bootstrapSample(dataFrame, bootstrapSize):\n",
        "    randomIndices = numpy.random.randint(low = 0, high = len(dataFrame), size = bootstrapSize)\n",
        "    return dataFrame.iloc[randomIndices]\n",
        "\n",
        "\n",
        "\n",
        "def createRandomForest(dataFrame, bootstrapSize, randomAttributes, randomSplits, forestSize = 20, treeMaxDepth = 1000):\n",
        "    forest = []\n",
        "    for i in prange(forestSize): # // hóa với prange thay range\n",
        "        bootstrappedDataFrame = bootstrapSample(dataFrame, bootstrapSize)\n",
        "        decisionTree = buildDecisionTree(bootstrappedDataFrame, maxDepth = treeMaxDepth, randomAttributes = randomAttributes, randomSplits = randomSplits)\n",
        "        forest.append(decisionTree)\n",
        "    return forest\n",
        "\n",
        "\n",
        "\n",
        "def randomForestPredictions(dataFrame, randomForest):\n",
        "    predictions = {}\n",
        "    for i in prange(len(randomForest)): # // hóa với prange thay range\n",
        "        column = \"decision tree \" + str(i)\n",
        "        predictions[column] = decisionTreePredictions(dataFrame, randomForest[i])\n",
        "    predictions = pandas.DataFrame(predictions)\n",
        "    return predictions.mode(axis = 1)[0]\n",
        "\n",
        "def calculateAccuracy(predictedResults, category):\n",
        "    resultCorrect = predictedResults == category\n",
        "    return resultCorrect.mean()"
      ],
      "metadata": {
        "id": "Q0Tq7p3rfuiw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sample run"
      ],
      "metadata": {
        "id": "k-3YHL-ofuiw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the dataset\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/Bài Tập Về Nhà Môn Cuối/iris.csv\")\n",
        "\n",
        "# label encoder\n",
        "from sklearn import preprocessing\n",
        "\n",
        "label_encoder = preprocessing.LabelEncoder()\n",
        "\n",
        "# Encode labels in column 'species'.\n",
        "df['species']= label_encoder.fit_transform(df['species'])\n",
        "\n",
        "%time\n",
        "randomForest = createRandomForest(df, bootstrapSize = 10, randomAttributes = 10, randomSplits = 50, forestSize = 30, treeMaxDepth = 3)\n",
        "randomForestTestResults = randomForestPredictions(df, randomForest)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 884
        },
        "outputId": "713b31bb-7421-4f88-e646-1fcf17c46c54",
        "id": "AVqlf7vrfuiw"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 3 µs, sys: 1 µs, total: 4 µs\n",
            "Wall time: 7.15 µs\n",
            "dataBelow=array([[4.4, 2.9, 1.4, 0.2, 0. ],\n",
            "       [5.4, 3.4, 1.5, 0.4, 0. ],\n",
            "       [5.8, 2.7, 3.9, 1.2, 1. ],\n",
            "       [5.5, 2.4, 3.7, 1. , 1. ]])\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypingError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypingError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-75-0c69ccc42a61>\u001b[0m in \u001b[0;36m<cell line: 13>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'time'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mrandomForest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreateRandomForest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbootstrapSize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandomAttributes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandomSplits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforestSize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtreeMaxDepth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0mrandomForestTestResults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandomForestPredictions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandomForest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-74-ccf2364961fc>\u001b[0m in \u001b[0;36mcreateRandomForest\u001b[0;34m(dataFrame, bootstrapSize, randomAttributes, randomSplits, forestSize, treeMaxDepth)\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mforestSize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# // hóa với prange thay range\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mbootstrappedDataFrame\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbootstrapSample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataFrame\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbootstrapSize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0mdecisionTree\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuildDecisionTree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbootstrappedDataFrame\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxDepth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtreeMaxDepth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandomAttributes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandomAttributes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandomSplits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandomSplits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0mforest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecisionTree\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mforest\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-73-6beab6a3db19>\u001b[0m in \u001b[0;36mbuildDecisionTree\u001b[0;34m(dataFrame, currentDepth, minSampleSize, maxDepth, randomAttributes, randomSplits)\u001b[0m\n\u001b[1;32m    172\u001b[0m         \u001b[0mcurrentDepth\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m         \u001b[0mpotentialSplits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetPotentialSplits\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandomAttributes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 174\u001b[0;31m         \u001b[0msplitColumn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplitValue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdetermineBestSplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpotentialSplits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandomSplits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    175\u001b[0m         \u001b[0mdataBelow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataAbove\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msplitData\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplitColumn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplitValue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataBelow\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataAbove\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-73-6beab6a3db19>\u001b[0m in \u001b[0;36mdetermineBestSplit\u001b[0;34m(data, potentialSplits, randomSplits)\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m             \u001b[0;31m# print(f'{randomSplitValue=}\\n{dataBelow=}\\n {dataAbove=}\\n\\n')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m             \u001b[0mcurrentOverallEntropy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalculateOverallEntropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataBelow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataAbove\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    150\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mcurrentOverallEntropy\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0moverallEntropy\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m                 \u001b[0moverallEntropy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcurrentOverallEntropy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-73-6beab6a3db19>\u001b[0m in \u001b[0;36mcalculateOverallEntropy\u001b[0;34m(dataBelow, dataAbove)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m     \u001b[0mcalculateEntropy\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mgridSizeBelow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblockSize\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBelow_uniqueClassesCounts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataBelow\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mentropyBelow\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m     \u001b[0mcalculateEntropy\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mgridSizeAbove\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblockSize\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mAbove_uniqueClassesCounts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataAbove\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mentropyAbove\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numba/cuda/dispatcher.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    489\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 491\u001b[0;31m         return self.dispatcher.call(args, self.griddim, self.blockdim,\n\u001b[0m\u001b[1;32m    492\u001b[0m                                     self.stream, self.sharedmem)\n\u001b[1;32m    493\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numba/cuda/dispatcher.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, args, griddim, blockdim, stream, sharedmem)\u001b[0m\n\u001b[1;32m    623\u001b[0m             \u001b[0mkernel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moverloads\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    624\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 625\u001b[0;31m             \u001b[0mkernel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_dispatcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDispatcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cuda_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    626\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    627\u001b[0m         \u001b[0mkernel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlaunch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgriddim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblockdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msharedmem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numba/cuda/dispatcher.py\u001b[0m in \u001b[0;36m_compile_for_args\u001b[0;34m(self, *args, **kws)\u001b[0m\n\u001b[1;32m    631\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mkws\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    632\u001b[0m         \u001b[0margtypes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtypeof_pyval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 633\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margtypes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    634\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    635\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtypeof_pyval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numba/cuda/dispatcher.py\u001b[0m in \u001b[0;36mcompile\u001b[0;34m(self, sig)\u001b[0m\n\u001b[1;32m    792\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Compilation disabled\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    793\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 794\u001b[0;31m             \u001b[0mkernel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Kernel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpy_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margtypes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtargetoptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    795\u001b[0m             \u001b[0;31m# We call bind to force codegen, so that there is a cubin to cache\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    796\u001b[0m             \u001b[0mkernel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numba/core/compiler_lock.py\u001b[0m in \u001b[0;36m_acquire_compile_lock\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0m_acquire_compile_lock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_acquire_compile_lock\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numba/cuda/dispatcher.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, py_func, argtypes, link, debug, lineinfo, inline, fastmath, extensions, max_registers, opt, device)\u001b[0m\n\u001b[1;32m     73\u001b[0m         }\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m         cres = compile_cuda(self.py_func, types.void, self.argtypes,\n\u001b[0m\u001b[1;32m     76\u001b[0m                             \u001b[0mdebug\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m                             \u001b[0mlineinfo\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlineinfo\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numba/core/compiler_lock.py\u001b[0m in \u001b[0;36m_acquire_compile_lock\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0m_acquire_compile_lock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_acquire_compile_lock\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numba/cuda/compiler.py\u001b[0m in \u001b[0;36mcompile_cuda\u001b[0;34m(pyfunc, return_type, args, debug, lineinfo, inline, fastmath, nvvm_options)\u001b[0m\n\u001b[1;32m    210\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mnumba\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_extension\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtarget_override\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtarget_override\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cuda'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 212\u001b[0;31m         cres = compiler.compile_extra(typingctx=typingctx,\n\u001b[0m\u001b[1;32m    213\u001b[0m                                       \u001b[0mtargetctx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtargetctx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m                                       \u001b[0mfunc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpyfunc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numba/core/compiler.py\u001b[0m in \u001b[0;36mcompile_extra\u001b[0;34m(typingctx, targetctx, func, args, return_type, flags, locals, library, pipeline_class)\u001b[0m\n\u001b[1;32m    714\u001b[0m     pipeline = pipeline_class(typingctx, targetctx, library,\n\u001b[1;32m    715\u001b[0m                               args, return_type, flags, locals)\n\u001b[0;32m--> 716\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile_extra\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    717\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    718\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numba/core/compiler.py\u001b[0m in \u001b[0;36mcompile_extra\u001b[0;34m(self, func)\u001b[0m\n\u001b[1;32m    450\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlifted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlifted_from\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 452\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compile_bytecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    453\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    454\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcompile_ir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc_ir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlifted\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlifted_from\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numba/core/compiler.py\u001b[0m in \u001b[0;36m_compile_bytecode\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    518\u001b[0m         \"\"\"\n\u001b[1;32m    519\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc_ir\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 520\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compile_core\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    522\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_compile_ir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numba/core/compiler.py\u001b[0m in \u001b[0;36m_compile_core\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    497\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfail_reason\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    498\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mis_final_pipeline\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 499\u001b[0;31m                         \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    500\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    501\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mCompilerError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"All available pipelines exhausted\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numba/core/compiler.py\u001b[0m in \u001b[0;36m_compile_core\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    484\u001b[0m                 \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    485\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 486\u001b[0;31m                     \u001b[0mpm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    487\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcr\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m                         \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numba/core/compiler_machinery.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m    366\u001b[0m                     \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpipeline_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpass_desc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    367\u001b[0m                 \u001b[0mpatched_exception\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_patch_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 368\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mpatched_exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    369\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    370\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdependency_analysis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numba/core/compiler_machinery.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m    354\u001b[0m                 \u001b[0mpass_inst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_pass_registry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpass_inst\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    355\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpass_inst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCompilerPass\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 356\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_runPass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpass_inst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    357\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mBaseException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Legacy pass in use\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numba/core/compiler_lock.py\u001b[0m in \u001b[0;36m_acquire_compile_lock\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0m_acquire_compile_lock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_acquire_compile_lock\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numba/core/compiler_machinery.py\u001b[0m in \u001b[0;36m_runPass\u001b[0;34m(self, index, pss, internal_state)\u001b[0m\n\u001b[1;32m    309\u001b[0m                 \u001b[0mmutated\u001b[0m \u001b[0;34m|=\u001b[0m \u001b[0mcheck\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_initialization\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minternal_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mSimpleTimer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpass_time\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 311\u001b[0;31m                 \u001b[0mmutated\u001b[0m \u001b[0;34m|=\u001b[0m \u001b[0mcheck\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_pass\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minternal_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mSimpleTimer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfinalize_time\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m                 \u001b[0mmutated\u001b[0m \u001b[0;34m|=\u001b[0m \u001b[0mcheck\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_finalizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minternal_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numba/core/compiler_machinery.py\u001b[0m in \u001b[0;36mcheck\u001b[0;34m(func, compiler_state)\u001b[0m\n\u001b[1;32m    271\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcheck\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompiler_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 273\u001b[0;31m             \u001b[0mmangled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompiler_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    274\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmangled\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m                 msg = (\"CompilerPass implementations should return True/False. \"\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numba/core/typed_passes.py\u001b[0m in \u001b[0;36mrun_pass\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m    103\u001b[0m                               % (state.func_id.func_name,)):\n\u001b[1;32m    104\u001b[0m             \u001b[0;31m# Type inference\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m             typemap, return_type, calltypes, errs = type_inference_stage(\n\u001b[0m\u001b[1;32m    106\u001b[0m                 \u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtypingctx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m                 \u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtargetctx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numba/core/typed_passes.py\u001b[0m in \u001b[0;36mtype_inference_stage\u001b[0;34m(typingctx, targetctx, interp, args, return_type, locals, raise_errors)\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0minfer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_constraint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0;31m# return errors in case of partial typing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m         \u001b[0merrs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minfer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpropagate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraise_errors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mraise_errors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m         \u001b[0mtypemap\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrestype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcalltypes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minfer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraise_errors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mraise_errors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numba/core/typeinfer.py\u001b[0m in \u001b[0;36mpropagate\u001b[0;34m(self, raise_errors)\u001b[0m\n\u001b[1;32m   1084\u001b[0m                                   if isinstance(e, ForceLiteralArg)]\n\u001b[1;32m   1085\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mforce_lit_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1086\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1087\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1088\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moperator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mor_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_lit_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypingError\u001b[0m: Failed in cuda mode pipeline (step: nopython frontend)\nNo implementation of function Function(<built-in function setitem>) found for signature:\n \n >>> setitem(array(int64, 0d, C), int32, float64)\n \nThere are 16 candidate implementations:\n      - Of which 14 did not match due to:\n      Overload of function 'setitem': File: <numerous>: Line N/A.\n        With argument(s): '(array(int64, 0d, C), int32, float64)':\n       No match.\n      - Of which 2 did not match due to:\n      Overload in function 'SetItemBuffer.generic': File: numba/core/typing/arraydecl.py: Line 176.\n        With argument(s): '(array(int64, 0d, C), int32, float64)':\n       Rejected as the implementation raised a specific error:\n         NumbaTypeError: cannot index array(int64, 0d, C) with 1 indices: [int32]\n  raised from /usr/local/lib/python3.10/dist-packages/numba/core/typing/arraydecl.py:88\n\nDuring: typing of setitem at <ipython-input-73-6beab6a3db19> (84)\n\nFile \"<ipython-input-73-6beab6a3db19>\", line 84:\ndef calculateEntropy(uniqueClassesCounts, lendata, result):\n    <source elided>\n\n    result[idx] = 0.0\n    ^\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy = calculateAccuracy(randomForestTestResults, df.iloc[:, -1]) * 100\n",
        "accuracy"
      ],
      "metadata": {
        "id": "v6Q6wdFdfuix"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import numba.cuda as cuda\n",
        "\n",
        "# Define a CUDA kernel function using numba.cuda.jit\n",
        "@cuda.jit\n",
        "def square_array(arr):\n",
        "    # Calculate the global index of the current thread\n",
        "    idx = cuda.grid(1)\n",
        "\n",
        "    # Perform the computation\n",
        "    if idx < arr.size:\n",
        "        arr[idx] = arr[idx] ** 2\n",
        "\n",
        "# Create an input array\n",
        "data = np.arange(10, dtype=np.float32)\n",
        "\n",
        "# Transfer the input array to the GPU\n",
        "d_data = cuda.to_device(data)\n",
        "\n",
        "# Configure the kernel launch\n",
        "threads_per_block = 64\n",
        "blocks_per_grid = (data.size + (threads_per_block - 1)) // threads_per_block\n",
        "\n",
        "# Launch the CUDA kernel\n",
        "square_array[blocks_per_grid, threads_per_block](d_data)\n",
        "\n",
        "# Transfer the modified array back to the host\n",
        "result = d_data.copy_to_host()\n",
        "\n",
        "# Print the result\n",
        "print(result)\n"
      ],
      "metadata": {
        "id": "QdStqX9yOhnq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "dt = np.array([[1, 2, 7],\n",
        "               [3, 4, 8],\n",
        "               [5, 6, 9],\n",
        "               [11, 12, 13]])\n",
        "\n",
        "psl = getPotentialSplits(dt, [0,1])\n",
        "\n"
      ],
      "metadata": {
        "id": "DNrX_wq1wFgm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "t, uniqueClassesCounts = np.unique(dt[:, -1], return_counts = True)"
      ],
      "metadata": {
        "id": "OyYUmvXmqInm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "uniqueClassesCounts"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X36EWozVz_GH",
        "outputId": "fb724cb1-27bb-4fbe-e5d1-8cfe6d1ac544"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 1, 1, 1])"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from numba import cuda\n",
        "\n",
        "@cuda.reduce\n",
        "def my_sum(a, b):\n",
        "    return a + b\n",
        "\n",
        "@cuda.jit\n",
        "def my_cuda_kernel(data, res):\n",
        "    i, j = cuda.grid(2)\n",
        "    if i < data.shape[0] and j < data.shape[1]:\n",
        "        # Perform calculations or assignments\n",
        "        row_sum = my_sum(data[i, :])  # Calculate sum along rows\n",
        "        col_sum = my_sum(data[:, j])  # Calculate sum along columns\n",
        "        # Perform further computations or assignments with the row_sum and col_sum\n",
        "    res[i, j] = row_sum\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "from numba import cuda\n",
        "\n",
        "# Generate sample data\n",
        "# data = np.random.rand(10, 10)\n",
        "\n",
        "res = 0\n",
        "\n",
        "# Define CUDA kernel configuration\n",
        "threads_per_block = (16, 16)\n",
        "blocks_per_grid_x = (dt.shape[0] + threads_per_block[0] - 1) // threads_per_block[0]\n",
        "blocks_per_grid_y = (dt.shape[1] + threads_per_block[1] - 1) // threads_per_block[1]\n",
        "blocks_per_grid = (blocks_per_grid_x, blocks_per_grid_y)\n",
        "\n",
        "# Allocate device memory for the data\n",
        "d_data = cuda.to_device(dt)\n",
        "d_res = cuda.to_device(res)\n",
        "\n",
        "# Launch the CUDA kernel\n",
        "my_cuda_kernel[blocks_per_grid, threads_per_block](d_data, d_res)\n",
        "\n",
        "# Copy the results back to the host\n",
        "result_data = d_data.copy_to_host()\n",
        "res_res = d_res.copy_to_host()\n",
        "\n",
        "print(result_data)\n"
      ],
      "metadata": {
        "id": "8xGdYIGdqMBW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from numba import cuda\n",
        "\n",
        "@cuda.jit\n",
        "def sum_array_parallel(array, result):\n",
        "    _,i = cuda.grid(2)\n",
        "    if i < array.shape[1]:\n",
        "        result[i] -= 1\n",
        "\n",
        "# Generate sample data\n",
        "data = dt\n",
        "\n",
        "# Define CUDA kernel configuration\n",
        "block_size = 8\n",
        "num_blocks = (data.shape[0] + block_size - 1) // block_size\n",
        "\n",
        "# Allocate device memory for the data and result\n",
        "d_data = cuda.to_device(data)\n",
        "result = np.array([0], dtype=np.int32)\n",
        "d_result = cuda.to_device(result)\n",
        "\n",
        "# Launch the CUDA kernel\n",
        "sum_array_parallel[num_blocks, block_size](d_data, d_result)\n",
        "\n",
        "# Copy the result back to the host\n",
        "result = d_result.copy_to_host()\n",
        "\n",
        "print(result[0])  # Output: 45 (sum of elements in the array)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2dEGr_MiyAPu",
        "outputId": "71bceff7-cfa4-4324-d6cb-53296f08682f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "psl"
      ],
      "metadata": {
        "id": "sEcG7askzihW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "determineBestSplit(dt, psl, None)"
      ],
      "metadata": {
        "id": "JY1eTD0Cw-2b"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}