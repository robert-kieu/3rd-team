{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "eR7caJRLN6X4",
        "51jRU8qnRRNW",
        "w6TjupWzCZ1R",
        "4mSWkbQ_AFof"
      ],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "kJHUxPPiFbTQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# To ignore warinings\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "rhzkSx_JSfRl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1ehfGR5ZhmLU",
        "outputId": "3aee8b57-9b76-42a4-9b02-32a7794410ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Random forest là gì ?"
      ],
      "metadata": {
        "id": "A5ZduScIKvgX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Định nghĩa."
      ],
      "metadata": {
        "id": "DL3eA5s2RMSs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Random forest là một phương pháp thống kê mô hình hóa bằng máy (machine learning statistic) dùng để phục vụ các mục đích phân loại, hồi quy và các nhiệm vụ khác bằng cách xây dựng nhiều cây quyết định (Decision tree).\n",
        "\n",
        "[_Nguồn_](https://jgac.vn/journal/article/view/344#:~:text=Random%20forest%20l%C3%A0%20m%E1%BB%99t%20ph%C6%B0%C6%A1ng,quy%E1%BA%BFt%20%C4%91%E1%BB%8Bnh%20(Decision%20tree).)\n",
        "\n",
        "Random Forest được ghi nhận hiệu quả hơn so với thuật toán phân loại khác thường được sử dụng vì có khả năng tìm ra thuộc tính nào quan trọng hơn so với những thuộc tính khác.\n",
        "\n",
        "</br>\n",
        "\n",
        "![](https://res.cloudinary.com/dyd911kmh/image/upload/v1677239993/image5_c214968fd6.png)\n",
        "\n",
        "<br>\n",
        "\n",
        "**Ưu điểm:**\n",
        "\n",
        "Random forests được coi là một phương pháp chính xác và mạnh mẽ vì số cây quyết định tham gia vào quá trình này. Thuật toán có thể được sử dụng trong cả hai vấn đề phân loại và hồi quy.\n",
        "\n",
        "<br>\n",
        "\n",
        "**Nhược điểm:**\n",
        "\n",
        "Random forests chậm tạo dự đoán bởi vì nó có nhiều cây quyết định. Bất cứ khi nào nó đưa ra dự đoán, tất cả các cây trong rừng phải đưa ra dự đoán cho cùng một đầu vào cho trước và sau đó thực hiện bỏ phiếu trên đó. Toàn bộ quá trình này tốn rất nhiều thời gian.\n",
        "\n",
        "Mô hình khó hiểu hơn so với cây quyết định, nơi ta có thể dễ dàng đưa ra quyết định bằng cách đi theo đường dẫn trong cây."
      ],
      "metadata": {
        "id": "vVF57YJxKnDP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cách thức hoạt động."
      ],
      "metadata": {
        "id": "eR7caJRLN6X4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Chọn các mẫu ngẫu nhiên từ tập dữ liệu đã cho.\n",
        "\n",
        "2. Thiết lập cây quyết định cho từng mẫu và nhận kết quả dự đoán từ mỗi quyết định cây.\n",
        "\n",
        "3. Bỏ phiếu cho mỗi kết quả dự đoán.\n",
        "\n",
        "4. Chọn kết quả được dự đoán nhiều nhất làm dự đoán cuối cùng.\n",
        "\n",
        "<br>\n",
        "\n",
        "![](https://images.viblo.asia/33adb558-67ba-4262-809e-3f8a8348e0c8.png)"
      ],
      "metadata": {
        "id": "uGUNsC3lN-MM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ứng dụng thực tế."
      ],
      "metadata": {
        "id": "0W1PgE0pMqNd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Chủ yếu áp dụng vào việc giải các bài toán liên quan đến phân loại và hồi quy.\n",
        "\n",
        "<br>\n",
        "\n",
        "**Ví dụ:** Phân loại bệnh hoặc dự đoán giá cả hàng hóa trong tương lai."
      ],
      "metadata": {
        "id": "JmyiR-gVN5WA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hướng phát triển của nhóm."
      ],
      "metadata": {
        "id": "51jRU8qnRRNW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Áp dụng Numba, CUDA để chạy song song cho thuật toán Random forest nhằm mục đích cài thiện tốc độ trong quá trình sử dụng. Tiết kiệm thời gian huấn luyện và tận dụng tốt tài nguyên máy nhất có thể."
      ],
      "metadata": {
        "id": "IDh9aMAYRcva"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Nguồn tài liệu."
      ],
      "metadata": {
        "id": "w6TjupWzCZ1R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nhóm tìm được thuật toán Random Forest từ một [nguồn trên github](https://github.com/harrypnh/random-forest-from-scratch), họ implement thuật toán với 2 mục đích chính: chẩn đoán bệnh ung thư vú và đánh giá ô tô ( Car Evalution ).\n",
        "\n",
        "<br>\n",
        "\n",
        "**Tại sao lại chọn từ nguồn này?**\n",
        "\n",
        "Vì họ xây dựng thuật toán Random Forest theo hàm chứ không xây theo class, điều này khiến cho quá trình làm việc với numba trở nên dễ dàng hơn.\n"
      ],
      "metadata": {
        "id": "2tykp_EBC2EQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dữ liệu đầu vào."
      ],
      "metadata": {
        "id": "T4xfl4NsDDhG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Về phần dataset, có lẽ nhóm sẽ tìm một dataset nào đó đủ lớn và dễ hiểu để training model trước. Việc này sẽ dễ hơn là dùng lại dataset có sẵn vì các kiến thức về xe ô tô và y tế nhóm chưa có nhiều kinh nghiệm nên việc diễn giải sẽ khó hơn."
      ],
      "metadata": {
        "id": "EUw0852TDGmV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mục tiêu cần đạt được."
      ],
      "metadata": {
        "id": "TSRBREG_SoUc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "xây dựng ra tất cả 4 version:\n",
        "- V1 - là thuật toán Random Forest chạy tuần tự.\n",
        "- V2 - là thuật toán Random Forest chạy song song bằng CPU.\n",
        "- V3 - là thuật toán Random Forest chạy song song bằng GPU.\n",
        "- V4 - là thuật toán Random Forest chạy song song sử dụng shared memory.\n",
        "\n",
        "<br>\n",
        "\n",
        "Ở mức kỳ vọng 100% - nhóm mong muốn có thể xây dựng được thuật toán đến V3 ( GPU ) và tốc độ có sự cải thiện khi chạy với dataset đủ lớn.\n",
        "\n",
        "<br>\n",
        "\n",
        "Ở mức kỳ vọng 125% - nhóm mong muốn xây dựng được thuật toán đến V4 và có thể ứng dụng thuật toán của nhóm vào để thử giải quyết một bài toán thực tế nào đó do các thành viên nhóm đề cử ( ví dụ chuẩn đoán bệnh ).\n"
      ],
      "metadata": {
        "id": "Ly7so6GaSrcl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Utility functions"
      ],
      "metadata": {
        "id": "Lb2v_108BJ4l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Hàm hỗ trợ phân tách tập train - test\n",
        "\n",
        "def trainTestSplit(dataFrame, testSize):\n",
        "    if isinstance(testSize, float):\n",
        "        testSize = round(testSize * len(dataFrame))\n",
        "    indices = dataFrame.index.tolist()\n",
        "    testIndices = random.sample(population = indices, k = testSize)\n",
        "    dataFrameTest = dataFrame.loc[testIndices]\n",
        "    dataFrameTrain = dataFrame.drop(testIndices)\n",
        "    return dataFrameTrain, dataFrameTest"
      ],
      "metadata": {
        "id": "r7Yqcd5qBITP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Khái quát"
      ],
      "metadata": {
        "id": "XPcf3QDlG_mX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Để xây dựng thuật toán Random forest ( tuần tự ) thì ở đây, nhóm cần build 2 phần từ là:\n",
        "\n",
        "1. **DecisionTree:** Xử lý các vấn đề liên quan đến cây quyết định.\n",
        "\n",
        "2. **RandomForest:** Xử lý các vấn đề liên quan đến random forest.\n",
        "\n"
      ],
      "metadata": {
        "id": "luRJL1vFGn_G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Random Forest tuần tự **[ V1 ]**"
      ],
      "metadata": {
        "id": "ovLKUrPzF2fj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Decision Tree"
      ],
      "metadata": {
        "id": "dfCV_YOsEraH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy\n",
        "import pandas\n",
        "import random\n",
        "\n",
        "\n",
        "'''\n",
        "Hàm này kiểm tra xem có tiếp tục còn nhánh để rẽ nữa hay k,\n",
        "nếu k thì trả về True, ngược lại trả về False.\n",
        "'''\n",
        "\n",
        "def checkPurity(data):\n",
        "    if len(numpy.unique(data[:, -1])) == 1:\n",
        "        return True\n",
        "    else:\n",
        "        return False\n",
        "\n",
        "\n",
        "'''\n",
        "Hàm này kiểm tra tất cả các nhãn cuối cùng của các cây quyết định,\n",
        "sau đó chọn ra nhãn xuất hiện nhiều nhất và gắn cho đối tượng cần\n",
        "dự đoán.\n",
        "'''\n",
        "\n",
        "def classifyData(data):\n",
        "    uniqueClasses, uniqueClassesCounts = numpy.unique(data[:, -1], return_counts = True)\n",
        "    return uniqueClasses[uniqueClassesCounts.argmax()]\n",
        "\n",
        "\n",
        "'''\n",
        "Hàm này mực đích tìm ra các ứng cử viên sáng giá cho việc rẽ nhánh. Nhận vào\n",
        "data và randomAttributes.\n",
        "\n",
        "Hàm sẽ kiểm tra như comment:\n",
        "'''\n",
        "\n",
        "def getPotentialSplits(data, randomAttributes):\n",
        "    potentialSplits = {}\n",
        "    _, columns = data.shape\n",
        "\n",
        "    # lấy ra số lượng cột có trong data.\n",
        "    columnsIndices = list(range(columns - 1))\n",
        "\n",
        "    # kiểm tra nếu randomAttributes != None & số lượng randomAttributes <=\n",
        "    # số cột tối đa của data truyền vào thì gán\n",
        "    # columnsIndices = randomAttributes\n",
        "    if randomAttributes != None  and len(randomAttributes) <= len(columnsIndices):\n",
        "        columnsIndices = randomAttributes\n",
        "\n",
        "    # với mỗi cột trong columnsIndices, ta lấy ra tất cả dòng giá trị của cột đó\n",
        "    # và lọc ra các giá trị độc nhất.\n",
        "    # nếu chỉ có duy nhất 1 giá trị độc nhất thì ra gán\n",
        "    # potentialSplits[column] = uniqueValues tức ứng cử viên cho cột ta vừa chọn\n",
        "    # là giá trị đó.\n",
        "    for column in columnsIndices:\n",
        "        values = data[:, column]\n",
        "        uniqueValues = numpy.unique(values)\n",
        "        if len(uniqueValues) == 1:\n",
        "            potentialSplits[column] = uniqueValues\n",
        "        else:\n",
        "            potentialSplits[column] = []\n",
        "            for i in range(len(uniqueValues)):\n",
        "                if i != 0:\n",
        "                    currentValue = uniqueValues[i]\n",
        "                    previousValue = uniqueValues[i - 1]\n",
        "                    potentialSplits[column].append((currentValue + previousValue) / 2)\n",
        "    return potentialSplits\n",
        "\n",
        "'''\n",
        "\n",
        "'''\n",
        "\n",
        "def splitData(data, splitColumn, splitValue):\n",
        "    splitColumnValues = data[:, splitColumn]\n",
        "    return data[splitColumnValues <= splitValue], data[splitColumnValues > splitValue]\n",
        "\n",
        "\n",
        "# //\n",
        "def calculateEntropy(data):\n",
        "    _, uniqueClassesCounts = numpy.unique(data[:, -1], return_counts = True)\n",
        "    probabilities = uniqueClassesCounts / uniqueClassesCounts.sum()\n",
        "    return sum(probabilities * -numpy.log2(probabilities))\n",
        "\n",
        "\n",
        "\n",
        "def calculateOverallEntropy(dataBelow, dataAbove):\n",
        "    pDataBelow = len(dataBelow) / (len(dataBelow) + len(dataAbove))\n",
        "    pDataAbove = len(dataAbove) / (len(dataBelow) + len(dataAbove))\n",
        "    return pDataBelow * calculateEntropy(dataBelow) + pDataAbove * calculateEntropy(dataAbove)\n",
        "\n",
        "\n",
        "\n",
        "def determineBestSplit(data, potentialSplits, randomSplits = None):\n",
        "    overallEntropy = 9999\n",
        "    bestSplitColumn = 0\n",
        "    bestSplitValue = 0\n",
        "    if randomSplits == None:\n",
        "        for splitColumn in potentialSplits:\n",
        "            for splitValue in potentialSplits[splitColumn]:\n",
        "                dataBelow, dataAbove = splitData(data, splitColumn, splitValue)\n",
        "                currentOverallEntropy = calculateOverallEntropy(dataBelow, dataAbove)\n",
        "                if currentOverallEntropy <= overallEntropy:\n",
        "                    overallEntropy = currentOverallEntropy\n",
        "                    bestSplitColumn = splitColumn\n",
        "                    bestSplitValue = splitValue\n",
        "    else:\n",
        "        for i in range(randomSplits): # //\n",
        "            randomSplitColumn = random.choice(list(potentialSplits))\n",
        "            randomSplitValue = random.choice(potentialSplits[randomSplitColumn])\n",
        "            dataBelow, dataAbove = splitData(data, randomSplitColumn, randomSplitValue)\n",
        "            currentOverallEntropy = calculateOverallEntropy(dataBelow, dataAbove)\n",
        "            if currentOverallEntropy <= overallEntropy:\n",
        "                overallEntropy = currentOverallEntropy\n",
        "                bestSplitColumn = randomSplitColumn\n",
        "                bestSplitValue = randomSplitValue\n",
        "    return bestSplitColumn, bestSplitValue\n",
        "\n",
        "\n",
        "'''\n",
        "Hàm này dùng để xây dựng cây quyết định. Nhận vào các tham số: data,\n",
        "độ sâu hiện tại, số lượng mẫu tối thiểu để tiếp tục phân tách,\n",
        "độ sâu tối đa, số lượng mẫu tối thiểu để tiếp tục split,\n",
        "số lần phân tách ngẫu nhiên.\n",
        "'''\n",
        "\n",
        "def buildDecisionTree(dataFrame, currentDepth = 0, minSampleSize = 2, maxDepth = 1000, randomAttributes = None, randomSplits = None):\n",
        "\n",
        "\n",
        "    # Nếu độ sâu hiện tại = 0, ta kiểm tra xem:\n",
        "\n",
        "    # Nếu randomAttributes != None & randomAttributes <= số lượng các cột - cột label thì\n",
        "    # ta gán randomAttributes = các cột bốc một cách ngẫu nhiên bằng hàm random.\n",
        "\n",
        "    # Ngược lại, randomAttributes = None.\n",
        "\n",
        "    if currentDepth == 0:\n",
        "        global COLUMN_HEADERS\n",
        "        COLUMN_HEADERS = dataFrame.columns\n",
        "        data = dataFrame.values\n",
        "        if randomAttributes != None and randomAttributes <= len(COLUMN_HEADERS) - 1:\n",
        "            randomAttributes = random.sample(population = list(range(len(COLUMN_HEADERS) - 1)), k = randomAttributes)\n",
        "        else:\n",
        "            randomAttributes = None\n",
        "\n",
        "\n",
        "    # Với độ sâu != 0, ta kiểm tra xem nếu hội tự các yếu tố: Purity = True hoặc\n",
        "    # số lượng mẫu ít hơn số lượng mẫu tối thiểu hoặc\n",
        "    # độ sâu đã đạt tối đa\n",
        "    # ==> ta cho mô hình phân loại luôn.\n",
        "\n",
        "    else:\n",
        "        data = dataFrame\n",
        "    if checkPurity(data) or len(data) < minSampleSize or currentDepth == maxDepth:\n",
        "        return classifyData(data)\n",
        "\n",
        "    # Ngược lại, ta đi sâu hơn currentDepth += 1, lấy dánh sách các ứng cử viên\n",
        "    # sáng giá để rẽ nhánh.\n",
        "    else:\n",
        "        currentDepth += 1\n",
        "        potentialSplits = getPotentialSplits(data, randomAttributes)\n",
        "        splitColumn, splitValue = determineBestSplit(data, potentialSplits, randomSplits)\n",
        "        dataBelow, dataAbove = splitData(data, splitColumn, splitValue)\n",
        "        if len(dataBelow) == 0 or len(dataAbove) == 0:\n",
        "            return classifyData(data)\n",
        "        else:\n",
        "            question = str(COLUMN_HEADERS[splitColumn]) + \" <= \" + str(splitValue)\n",
        "            decisionSubTree = {question: []}\n",
        "            yesAnswer = buildDecisionTree(dataBelow, currentDepth, minSampleSize, maxDepth, randomAttributes, randomSplits)\n",
        "            noAnswer = buildDecisionTree(dataAbove, currentDepth, minSampleSize, maxDepth, randomAttributes, randomSplits)\n",
        "            if yesAnswer == noAnswer:\n",
        "                decisionSubTree = yesAnswer\n",
        "            else:\n",
        "                decisionSubTree[question].append(yesAnswer)\n",
        "                decisionSubTree[question].append(noAnswer)\n",
        "            return decisionSubTree\n",
        "\n",
        "\n",
        "\n",
        "def classifySample(sample, decisionTree):\n",
        "    if not isinstance(decisionTree, dict):\n",
        "        return decisionTree\n",
        "    question = list(decisionTree.keys())[0]\n",
        "    attribute, value = question.split(\" <= \")\n",
        "    if sample[attribute] <= float(value):\n",
        "        answer = decisionTree[question][0]\n",
        "    else:\n",
        "        answer = decisionTree[question][1]\n",
        "    return classifySample(sample, answer)\n",
        "\n",
        "\n",
        "\n",
        "def decisionTreePredictions(dataFrame, decisionTree):\n",
        "    predictions = dataFrame.apply(classifySample, axis = 1, args = (decisionTree,))\n",
        "    return predictions\n",
        "\n",
        "\n",
        "\n",
        "def calculateAccuracy(predictedResults, category):\n",
        "    resultCorrect = predictedResults == category\n",
        "    return resultCorrect.mean()"
      ],
      "metadata": {
        "id": "7Xc41kdFEk1H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Random forest"
      ],
      "metadata": {
        "id": "_FgwIc7ZEvHT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Lấy mẫu Bootstrap là một kỹ thuật cơ bản được sử dụng trong việc xây dựng các khu rừng ngẫu nhiên.\n",
        "\n",
        "# Kỹ thuật này sẽ giúp ta tạo ra các decision tree một cách đa dạng hơn.\n",
        "\n",
        "def bootstrapSample(dataFrame, bootstrapSize):\n",
        "    randomIndices = numpy.random.randint(low = 0, high = len(dataFrame), size = bootstrapSize)\n",
        "    return dataFrame.iloc[randomIndices]\n",
        "\n",
        "\n",
        "# randomAttributes: số lượng thuộc tính ngẫu nhiên đc chọn cho mỗi lần phân tách.\n",
        "#                   việc này giúp giảm overfiting & tăng tính đa dạng giữa các cây\n",
        "#                   trong rừng.\n",
        "\n",
        "# randomSplits:     chứa các splitting point cho mỗi randomAttributes --> đa dạng hóa,\n",
        "#                   hạn chế overfitting.\n",
        "def createRandomForest(dataFrame, bootstrapSize, randomAttributes, randomSplits, numTree = 20, maxDepth = 1000):\n",
        "    forest = []\n",
        "    for i in range(numTree):\n",
        "        bootstrappedDF = bootstrapSample(dataFrame, bootstrapSize)\n",
        "        decisionTree = buildDecisionTree(bootstrappedDF, maxDepth = maxDepth, randomAttributes = randomAttributes, randomSplits = randomSplits)\n",
        "        forest.append(decisionTree)\n",
        "    return forest\n",
        "\n",
        "\n",
        "\n",
        "# Gom lại hết các kết quả dự đoán từ các cây quyết định và lấy mode để  cho ra kq cuối cùng.\n",
        "def randomForestPredictions(dataFrame, randomForest):\n",
        "    predictions = {}\n",
        "    for i in range(len(randomForest)):\n",
        "        column = \"decision tree \" + str(i)\n",
        "        predictions[column] = decisionTreePredictions(dataFrame, randomForest[i])\n",
        "    predictions = pandas.DataFrame(predictions)\n",
        "    return predictions.mode(axis = 1)[0]\n",
        "\n",
        "def calculateAccuracy(predictedResults, category):\n",
        "    resultCorrect = predictedResults == category\n",
        "    return resultCorrect.mean()"
      ],
      "metadata": {
        "id": "uuULCgMAEwsI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sample run"
      ],
      "metadata": {
        "id": "T9VWgE7PE4En"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the dataset\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/Bài Tập Về Nhà Môn Cuối/iris.csv\")\n",
        "\n",
        "# label encoder\n",
        "from sklearn import preprocessing\n",
        "\n",
        "label_encoder = preprocessing.LabelEncoder()\n",
        "\n",
        "# Encode labels in column 'species'.\n",
        "df['species']= label_encoder.fit_transform(df['species'])\n",
        "\n",
        "%time\n",
        "randomForest = createRandomForest(df, bootstrapSize = 10, randomAttributes = 10, randomSplits = 50, numTree = 30, maxDepth = 3)\n",
        "randomForestTestResults = randomForestPredictions(df, randomForest)"
      ],
      "metadata": {
        "id": "zPLJtwMNiTzA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c0d21532-1b43-4442-c6ba-e67733f245e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 4 µs, sys: 0 ns, total: 4 µs\n",
            "Wall time: 7.39 µs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy = calculateAccuracy(randomForestTestResults, df.iloc[:, -1]) * 100"
      ],
      "metadata": {
        "id": "NVjBaZgujPQR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ygls9Kw4i468",
        "outputId": "74ecfef2-77d0-4e5c-879d-6d8d20a38202"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "96.0"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Song song hóa sử dụng CPU **[ V2 ]**"
      ],
      "metadata": {
        "id": "4mSWkbQ_AFof"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Decision Tree V2.0"
      ],
      "metadata": {
        "id": "3q2yu7OkAMbY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy\n",
        "import pandas\n",
        "import random\n",
        "from numba import njit, prange\n",
        "\n",
        "\n",
        "\n",
        "def checkPurity(data):\n",
        "    if len(numpy.unique(data[:, -1])) == 1:\n",
        "        return True\n",
        "    else:\n",
        "        return False\n",
        "\n",
        "\n",
        "\n",
        "def classifyData(data):\n",
        "    uniqueClasses, uniqueClassesCounts = numpy.unique(data[:, -1], return_counts = True)\n",
        "    return uniqueClasses[uniqueClassesCounts.argmax()]\n",
        "\n",
        "\n",
        "\n",
        "def getPotentialSplits(data, randomAttributes):\n",
        "    potentialSplits = {}\n",
        "    _, columns = data.shape\n",
        "    columnsIndices = list(range(columns - 1))\n",
        "    if randomAttributes != None  and len(randomAttributes) <= len(columnsIndices):\n",
        "        columnsIndices = randomAttributes\n",
        "    for column in columnsIndices:\n",
        "        values = data[:, column]\n",
        "        uniqueValues = numpy.unique(values)\n",
        "        if len(uniqueValues) == 1:\n",
        "            potentialSplits[column] = uniqueValues\n",
        "        else:\n",
        "            potentialSplits[column] = []\n",
        "            for i in prange(len(uniqueValues)): # // ở đây bằng prange\n",
        "                if i != 0:\n",
        "                    currentValue = uniqueValues[i]\n",
        "                    previousValue = uniqueValues[i - 1]\n",
        "                    potentialSplits[column].append((currentValue + previousValue) / 2)\n",
        "    return potentialSplits\n",
        "\n",
        "\n",
        "\n",
        "def splitData(data, splitColumn, splitValue):\n",
        "    splitColumnValues = data[:, splitColumn]\n",
        "    return data[splitColumnValues <= splitValue], data[splitColumnValues > splitValue]\n",
        "\n",
        "\n",
        "njit(parallel=True)\n",
        "def calculateEntropy(data):\n",
        "    _, uniqueClassesCounts = numpy.unique(data[:, -1], return_counts = True)\n",
        "    probabilities = uniqueClassesCounts / uniqueClassesCounts.sum()\n",
        "\n",
        "    # // ở đây, thay thế hàm sum có sẵn trong python bằng vòng lặp sử dụng prange\n",
        "    entropy = 0.0\n",
        "    for i in prange(len(probabilities)):\n",
        "        entropy -= probabilities[i] * np.log2(probabilities[i])\n",
        "    return entropy\n",
        "\n",
        "\n",
        "\n",
        "def calculateOverallEntropy(dataBelow, dataAbove):\n",
        "    pDataBelow = len(dataBelow) / (len(dataBelow) + len(dataAbove))\n",
        "    pDataAbove = len(dataAbove) / (len(dataBelow) + len(dataAbove))\n",
        "    return pDataBelow * calculateEntropy(dataBelow) + pDataAbove * calculateEntropy(dataAbove)\n",
        "\n",
        "\n",
        "\n",
        "def determineBestSplit(data, potentialSplits, randomSplits = None):\n",
        "    overallEntropy = 9999\n",
        "    bestSplitColumn = 0\n",
        "    bestSplitValue = 0\n",
        "    if randomSplits == None:\n",
        "        for splitColumn in potentialSplits:\n",
        "            for splitValue in potentialSplits[splitColumn]:\n",
        "                dataBelow, dataAbove = splitData(data, splitColumn, splitValue)\n",
        "                currentOverallEntropy = calculateOverallEntropy(dataBelow, dataAbove)\n",
        "                if currentOverallEntropy <= overallEntropy:\n",
        "                    overallEntropy = currentOverallEntropy\n",
        "                    bestSplitColumn = splitColumn\n",
        "                    bestSplitValue = splitValue\n",
        "    else:\n",
        "        for i in prange(randomSplits): # // vòng for này bằng prange\n",
        "            randomSplitColumn = random.choice(list(potentialSplits))\n",
        "            randomSplitValue = random.choice(potentialSplits[randomSplitColumn])\n",
        "            dataBelow, dataAbove = splitData(data, randomSplitColumn, randomSplitValue)\n",
        "            currentOverallEntropy = calculateOverallEntropy(dataBelow, dataAbove)\n",
        "            if currentOverallEntropy <= overallEntropy:\n",
        "                overallEntropy = currentOverallEntropy\n",
        "                bestSplitColumn = randomSplitColumn\n",
        "                bestSplitValue = randomSplitValue\n",
        "    return bestSplitColumn, bestSplitValue\n",
        "\n",
        "\n",
        "\n",
        "def buildDecisionTree(dataFrame, currentDepth = 0, minSampleSize = 2, maxDepth = 1000, randomAttributes = None, randomSplits = None):\n",
        "    if currentDepth == 0:\n",
        "        global COLUMN_HEADERS\n",
        "        COLUMN_HEADERS = dataFrame.columns\n",
        "        data = dataFrame.values\n",
        "        if randomAttributes != None and randomAttributes <= len(COLUMN_HEADERS) - 1:\n",
        "            randomAttributes = random.sample(population = list(range(len(COLUMN_HEADERS) - 1)), k = randomAttributes)\n",
        "        else:\n",
        "            randomAttributes = None\n",
        "    else:\n",
        "        data = dataFrame\n",
        "    if checkPurity(data) or len(data) < minSampleSize or currentDepth == maxDepth:\n",
        "        return classifyData(data)\n",
        "    else:\n",
        "        currentDepth += 1\n",
        "        potentialSplits = getPotentialSplits(data, randomAttributes)\n",
        "        splitColumn, splitValue = determineBestSplit(data, potentialSplits, randomSplits)\n",
        "        dataBelow, dataAbove = splitData(data, splitColumn, splitValue)\n",
        "        if len(dataBelow) == 0 or len(dataAbove) == 0:\n",
        "            return classifyData(data)\n",
        "        else:\n",
        "            question = str(COLUMN_HEADERS[splitColumn]) + \" <= \" + str(splitValue)\n",
        "            decisionSubTree = {question: []}\n",
        "            yesAnswer = buildDecisionTree(dataBelow, currentDepth, minSampleSize, maxDepth, randomAttributes, randomSplits)\n",
        "            noAnswer = buildDecisionTree(dataAbove, currentDepth, minSampleSize, maxDepth, randomAttributes, randomSplits)\n",
        "            if yesAnswer == noAnswer:\n",
        "                decisionSubTree = yesAnswer\n",
        "            else:\n",
        "                decisionSubTree[question].append(yesAnswer)\n",
        "                decisionSubTree[question].append(noAnswer)\n",
        "            return decisionSubTree\n",
        "\n",
        "\n",
        "\n",
        "def classifySample(sample, decisionTree):\n",
        "    if not isinstance(decisionTree, dict):\n",
        "        return decisionTree\n",
        "    question = list(decisionTree.keys())[0]\n",
        "    attribute, value = question.split(\" <= \")\n",
        "    if sample[attribute] <= float(value):\n",
        "        answer = decisionTree[question][0]\n",
        "    else:\n",
        "        answer = decisionTree[question][1]\n",
        "    return classifySample(sample, answer)\n",
        "\n",
        "\n",
        "\n",
        "def decisionTreePredictions(dataFrame, decisionTree):\n",
        "    predictions = dataFrame.apply(classifySample, axis = 1, args = (decisionTree,))\n",
        "    return predictions\n",
        "\n",
        "\n",
        "\n",
        "def calculateAccuracy(predictedResults, category):\n",
        "    resultCorrect = predictedResults == category\n",
        "    return resultCorrect.mean()"
      ],
      "metadata": {
        "id": "-7OZFpHQAMbc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Random Forest V2.0"
      ],
      "metadata": {
        "id": "t8DVIv2WdJwg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def trainTestSplit(dataFrame, testSize):\n",
        "    if isinstance(testSize, float):\n",
        "        testSize = round(testSize * len(dataFrame))\n",
        "    indices = dataFrame.index.tolist()\n",
        "    testIndices = random.sample(population = indices, k = testSize)\n",
        "    dataFrameTest = dataFrame.loc[testIndices]\n",
        "    dataFrameTrain = dataFrame.drop(testIndices)\n",
        "    return dataFrameTrain, dataFrameTest\n",
        "\n",
        "def bootstrapSample(dataFrame, bootstrapSize):\n",
        "    randomIndices = numpy.random.randint(low = 0, high = len(dataFrame), size = bootstrapSize)\n",
        "    return dataFrame.iloc[randomIndices]\n",
        "\n",
        "\n",
        "\n",
        "def createRandomForest(dataFrame, bootstrapSize, randomAttributes, randomSplits, forestSize = 20, treeMaxDepth = 1000):\n",
        "    forest = []\n",
        "    for i in prange(forestSize): # // hóa với prange thay range\n",
        "        bootstrappedDataFrame = bootstrapSample(dataFrame, bootstrapSize)\n",
        "        decisionTree = buildDecisionTree(bootstrappedDataFrame, maxDepth = treeMaxDepth, randomAttributes = randomAttributes, randomSplits = randomSplits)\n",
        "        forest.append(decisionTree)\n",
        "    return forest\n",
        "\n",
        "\n",
        "\n",
        "def randomForestPredictions(dataFrame, randomForest):\n",
        "    predictions = {}\n",
        "    for i in prange(len(randomForest)): # // hóa với prange thay range\n",
        "        column = \"decision tree \" + str(i)\n",
        "        predictions[column] = decisionTreePredictions(dataFrame, randomForest[i])\n",
        "    predictions = pandas.DataFrame(predictions)\n",
        "    return predictions.mode(axis = 1)[0]\n",
        "\n",
        "def calculateAccuracy(predictedResults, category):\n",
        "    resultCorrect = predictedResults == category\n",
        "    return resultCorrect.mean()"
      ],
      "metadata": {
        "id": "RXLvnU-RdRQZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sample run"
      ],
      "metadata": {
        "id": "aRD5t9c6dVez"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the dataset\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/Bài Tập Về Nhà Môn Cuối/iris.csv\")\n",
        "\n",
        "%time\n",
        "randomForest = createRandomForest(df, bootstrapSize = 10, randomAttributes = 10, randomSplits = 50, forestSize = 30, treeMaxDepth = 3)\n",
        "randomForestTestResults = randomForestPredictions(df, randomForest)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2vit1jh6dXlO",
        "outputId": "d9718ca5-46ee-48e7-a70d-5f9f5c300c9f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 3 µs, sys: 0 ns, total: 3 µs\n",
            "Wall time: 6.44 µs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy = calculateAccuracy(randomForestTestResults, df.iloc[:, -1]) * 100\n",
        "accuracy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QAgEou26daOU",
        "outputId": "f89f0b5a-c712-4d9d-b40b-75ae09cdd772"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "95.33333333333334"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Song song hóa sử dụng CPU & GPU **[ V3 ]**"
      ],
      "metadata": {
        "id": "6-vhmeHofuiP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Decision Tree V3.0"
      ],
      "metadata": {
        "id": "_MflX4LEfuit"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy\n",
        "import pandas\n",
        "import random\n",
        "from numba import njit, prange, cuda, float32, int32\n",
        "import numba\n",
        "\n",
        "\n",
        "\n",
        "'''\n",
        "\n",
        "Hàm bên dưới xác định \"độ tinh khiết (Pure)\" của tập dữ liệu bằng cách kiểm tra xem nó\n",
        "\n",
        "có chứa cùng một label hay không, nếu có thì return true, ngược lại return false.\n",
        "\n",
        "'''\n",
        "\n",
        "def checkPurity(data):\n",
        "    if len(numpy.unique(data[:, -1])) == 1:\n",
        "        return True\n",
        "    else:\n",
        "        return False\n",
        "\n",
        "    # ...\n",
        "\n",
        "\n",
        "\n",
        "def classifyData(data):\n",
        "    uniqueClasses, uniqueClassesCounts = numpy.unique(data[:, -1], return_counts = True)\n",
        "    return uniqueClasses[uniqueClassesCounts.argmax()]\n",
        "\n",
        "    # ...\n",
        "\n",
        "\n",
        "\n",
        "def getPotentialSplits(data, randomAttributes):\n",
        "    potentialSplits = {}\n",
        "    _, columns = data.shape\n",
        "    columnsIndices = list(range(columns - 1))\n",
        "    if randomAttributes != None  and len(randomAttributes) <= len(columnsIndices):\n",
        "        columnsIndices = randomAttributes\n",
        "    for column in columnsIndices:\n",
        "        values = data[:, column]\n",
        "        uniqueValues = numpy.unique(values)\n",
        "        if len(uniqueValues) == 1:\n",
        "            potentialSplits[column] = uniqueValues\n",
        "        else:\n",
        "            potentialSplits[column] = []\n",
        "            for i in prange(len(uniqueValues)): # // ở đây bằng prange\n",
        "                if i != 0:\n",
        "                    currentValue = uniqueValues[i]\n",
        "                    previousValue = uniqueValues[i - 1]\n",
        "                    potentialSplits[column].append((currentValue + previousValue) / 2) # tại sao dừng công thức này ?\n",
        "    return potentialSplits\n",
        "\n",
        "\n",
        "\n",
        "def splitData(data, splitColumn, splitValue):\n",
        "    splitColumnValues = data[:, splitColumn]\n",
        "    return data[splitColumnValues <= splitValue], data[splitColumnValues > splitValue]\n",
        "\n",
        "\n",
        "# CUDA kernel ref. https://nyu-cds.github.io/python-numba/05-cuda/\n",
        "@cuda.jit\n",
        "def calculateEntropy(data, result):\n",
        "\n",
        "    idx = cuda.grid(2)  # Get the thread index\n",
        "\n",
        "    if idx < len(data):\n",
        "      _, uniqueClassesCounts = numpy.unique(data[:, -1], return_counts = True)\n",
        "\n",
        "      probabilities = uniqueClassesCounts / uniqueClassesCounts.sum()\n",
        "\n",
        "      # // ở đây, thay thế hàm sum có sẵn trong python bằng vòng lặp sử dụng prange\n",
        "      entropy = 0.0\n",
        "      for i in prange(len(probabilities)):\n",
        "          entropy -= probabilities[i] * np.log2(probabilities[i])\n",
        "\n",
        "      result[idx] = entropy\n",
        "\n",
        "    # ...\n",
        "\n",
        "\n",
        "\n",
        "def calculateOverallEntropy(dataBelow, dataAbove):\n",
        "    # pDataBelow = len(dataBelow) / (len(dataBelow) + len(dataAbove))\n",
        "    # pDataAbove = len(dataAbove) / (len(dataBelow) + len(dataAbove))\n",
        "    # return pDataBelow * calculateEntropy(dataBelow) + pDataAbove * calculateEntropy(dataAbove)\n",
        "\n",
        "    pDataBelow = len(dataBelow) / (len(dataBelow) + len(dataAbove))\n",
        "    pDataAbove = len(dataAbove) / (len(dataBelow) + len(dataAbove))\n",
        "\n",
        "    # Allocate device memory for entropy calculation\n",
        "    entropyBelow = cuda.device_array(len(dataBelow), dtype=float)\n",
        "    entropyAbove = cuda.device_array(len(dataAbove), dtype=float)\n",
        "\n",
        "    # Launch CUDA kernels for entropy calculation\n",
        "    blockSize = 128\n",
        "    gridSizeBelow = (len(dataBelow) + blockSize - 1) // blockSize\n",
        "    gridSizeAbove = (len(dataAbove) + blockSize - 1) // blockSize\n",
        "    calculateEntropy[gridSizeBelow, blockSize](dataBelow, entropyBelow)\n",
        "    calculateEntropy[gridSizeAbove, blockSize](dataAbove, entropyAbove)\n",
        "\n",
        "    # Transfer results back to the host\n",
        "    entropyBelow = entropyBelow.copy_to_host()\n",
        "    entropyAbove = entropyAbove.copy_to_host()\n",
        "\n",
        "    overallEntropy = pDataBelow * entropyBelow.sum() + pDataAbove * entropyAbove.sum()\n",
        "    return overallEntropy\n",
        "\n",
        "    # ...\n",
        "\n",
        "\n",
        "def determineBestSplit(data, potentialSplits, randomSplits = None):\n",
        "    overallEntropy = 9999\n",
        "    bestSplitColumn = 0\n",
        "    bestSplitValue = 0\n",
        "    if randomSplits == None:\n",
        "        for splitColumn in potentialSplits:\n",
        "            for splitValue in potentialSplits[splitColumn]:\n",
        "                dataBelow, dataAbove = splitData(data, splitColumn, splitValue)\n",
        "                currentOverallEntropy = calculateOverallEntropy(dataBelow, dataAbove)\n",
        "                if currentOverallEntropy <= overallEntropy:\n",
        "                    overallEntropy = currentOverallEntropy\n",
        "                    bestSplitColumn = splitColumn\n",
        "                    bestSplitValue = splitValue\n",
        "    else:\n",
        "        for i in prange(randomSplits): # // vòng for này bằng prange\n",
        "            randomSplitColumn = random.choice(list(potentialSplits))\n",
        "            randomSplitValue = random.choice(potentialSplits[randomSplitColumn])\n",
        "            dataBelow, dataAbove = splitData(data, randomSplitColumn, randomSplitValue)\n",
        "\n",
        "            print(f'{randomSplitValue=}\\n{dataBelow=}\\n {dataAbove=}\\n\\n')\n",
        "            currentOverallEntropy = calculateOverallEntropy(dataBelow, dataAbove)\n",
        "            if currentOverallEntropy <= overallEntropy:\n",
        "                overallEntropy = currentOverallEntropy\n",
        "                bestSplitColumn = randomSplitColumn\n",
        "                bestSplitValue = randomSplitValue\n",
        "    return bestSplitColumn, bestSplitValue\n",
        "\n",
        "\n",
        "\n",
        "def buildDecisionTree(dataFrame, currentDepth = 0, minSampleSize = 2, maxDepth = 1000, randomAttributes = None, randomSplits = None):\n",
        "    if currentDepth == 0:\n",
        "        global COLUMN_HEADERS\n",
        "        COLUMN_HEADERS = dataFrame.columns\n",
        "        data = dataFrame.values\n",
        "        if randomAttributes != None and randomAttributes <= len(COLUMN_HEADERS) - 1:\n",
        "            randomAttributes = random.sample(population = list(range(len(COLUMN_HEADERS) - 1)), k = randomAttributes)\n",
        "        else:\n",
        "            randomAttributes = None\n",
        "    else:\n",
        "        data = dataFrame\n",
        "    if checkPurity(data) or len(data) < minSampleSize or currentDepth == maxDepth:\n",
        "        return classifyData(data)\n",
        "    else:\n",
        "        currentDepth += 1\n",
        "        potentialSplits = getPotentialSplits(data, randomAttributes)\n",
        "        splitColumn, splitValue = determineBestSplit(data, potentialSplits, randomSplits)\n",
        "        dataBelow, dataAbove = splitData(data, splitColumn, splitValue)\n",
        "        if len(dataBelow) == 0 or len(dataAbove) == 0:\n",
        "            return classifyData(data)\n",
        "        else:\n",
        "            question = str(COLUMN_HEADERS[splitColumn]) + \" <= \" + str(splitValue)\n",
        "            decisionSubTree = {question: []}\n",
        "            yesAnswer = buildDecisionTree(dataBelow, currentDepth, minSampleSize, maxDepth, randomAttributes, randomSplits)\n",
        "            noAnswer = buildDecisionTree(dataAbove, currentDepth, minSampleSize, maxDepth, randomAttributes, randomSplits)\n",
        "            if yesAnswer == noAnswer:\n",
        "                decisionSubTree = yesAnswer\n",
        "            else:\n",
        "                decisionSubTree[question].append(yesAnswer)\n",
        "                decisionSubTree[question].append(noAnswer)\n",
        "            return decisionSubTree\n",
        "\n",
        "\n",
        "\n",
        "def classifySample(sample, decisionTree):\n",
        "    if not isinstance(decisionTree, dict):\n",
        "        return decisionTree\n",
        "    question = list(decisionTree.keys())[0]\n",
        "    attribute, value = question.split(\" <= \")\n",
        "    if sample[attribute] <= float(value):\n",
        "        answer = decisionTree[question][0]\n",
        "    else:\n",
        "        answer = decisionTree[question][1]\n",
        "    return classifySample(sample, answer)\n",
        "\n",
        "\n",
        "\n",
        "def decisionTreePredictions(dataFrame, decisionTree):\n",
        "    predictions = dataFrame.apply(classifySample, axis = 1, args = (decisionTree,))\n",
        "    return predictions\n",
        "\n",
        "\n",
        "\n",
        "def calculateAccuracy(predictedResults, category):\n",
        "    resultCorrect = predictedResults == category\n",
        "    return resultCorrect.mean()"
      ],
      "metadata": {
        "id": "i4TBxO7Hfuit"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Random Forest V2.0"
      ],
      "metadata": {
        "id": "x7afKUaXfuiv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def trainTestSplit(dataFrame, testSize):\n",
        "    if isinstance(testSize, float):\n",
        "        testSize = round(testSize * len(dataFrame))\n",
        "    indices = dataFrame.index.tolist()\n",
        "    testIndices = random.sample(population = indices, k = testSize)\n",
        "    dataFrameTest = dataFrame.loc[testIndices]\n",
        "    dataFrameTrain = dataFrame.drop(testIndices)\n",
        "    return dataFrameTrain, dataFrameTest\n",
        "\n",
        "def bootstrapSample(dataFrame, bootstrapSize):\n",
        "    randomIndices = numpy.random.randint(low = 0, high = len(dataFrame), size = bootstrapSize)\n",
        "    return dataFrame.iloc[randomIndices]\n",
        "\n",
        "\n",
        "\n",
        "def createRandomForest(dataFrame, bootstrapSize, randomAttributes, randomSplits, forestSize = 20, treeMaxDepth = 1000):\n",
        "    forest = []\n",
        "    for i in prange(forestSize): # // hóa với prange thay range\n",
        "        bootstrappedDataFrame = bootstrapSample(dataFrame, bootstrapSize)\n",
        "        decisionTree = buildDecisionTree(bootstrappedDataFrame, maxDepth = treeMaxDepth, randomAttributes = randomAttributes, randomSplits = randomSplits)\n",
        "        forest.append(decisionTree)\n",
        "    return forest\n",
        "\n",
        "\n",
        "\n",
        "def randomForestPredictions(dataFrame, randomForest):\n",
        "    predictions = {}\n",
        "    for i in prange(len(randomForest)): # // hóa với prange thay range\n",
        "        column = \"decision tree \" + str(i)\n",
        "        predictions[column] = decisionTreePredictions(dataFrame, randomForest[i])\n",
        "    predictions = pandas.DataFrame(predictions)\n",
        "    return predictions.mode(axis = 1)[0]\n",
        "\n",
        "def calculateAccuracy(predictedResults, category):\n",
        "    resultCorrect = predictedResults == category\n",
        "    return resultCorrect.mean()"
      ],
      "metadata": {
        "id": "Q0Tq7p3rfuiw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sample run"
      ],
      "metadata": {
        "id": "k-3YHL-ofuiw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the dataset\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/Bài Tập Về Nhà Môn Cuối/iris.csv\")\n",
        "\n",
        "# label encoder\n",
        "from sklearn import preprocessing\n",
        "\n",
        "label_encoder = preprocessing.LabelEncoder()\n",
        "\n",
        "# Encode labels in column 'species'.\n",
        "df['species']= label_encoder.fit_transform(df['species'])\n",
        "\n",
        "%time\n",
        "randomForest = createRandomForest(df, bootstrapSize = 10, randomAttributes = 10, randomSplits = 50, forestSize = 30, treeMaxDepth = 3)\n",
        "randomForestTestResults = randomForestPredictions(df, randomForest)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "94098d8a-75ea-417f-9466-9674ddca59d6",
        "id": "AVqlf7vrfuiw"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
            "Wall time: 5.72 µs\n",
            "randomSplitValue=5.65\n",
            "dataBelow=array([[5. , 3.3, 1.4, 0.2, 0. ],\n",
            "       [5.2, 3.5, 1.5, 0.2, 0. ],\n",
            "       [5. , 3. , 1.6, 0.2, 0. ]])\n",
            " dataAbove=array([[7.7, 3. , 6.1, 2.3, 2. ],\n",
            "       [6.1, 3. , 4.9, 1.8, 2. ],\n",
            "       [7.7, 3.8, 6.7, 2.2, 2. ],\n",
            "       [6.6, 2.9, 4.6, 1.3, 1. ],\n",
            "       [6.3, 2.3, 4.4, 1.3, 1. ],\n",
            "       [6.4, 2.8, 5.6, 2.1, 2. ],\n",
            "       [6.9, 3.1, 4.9, 1.5, 1. ]])\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypingError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypingError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-0c69ccc42a61>\u001b[0m in \u001b[0;36m<cell line: 13>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'time'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mrandomForest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreateRandomForest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbootstrapSize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandomAttributes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandomSplits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforestSize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtreeMaxDepth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0mrandomForestTestResults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandomForestPredictions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandomForest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-15-ccf2364961fc>\u001b[0m in \u001b[0;36mcreateRandomForest\u001b[0;34m(dataFrame, bootstrapSize, randomAttributes, randomSplits, forestSize, treeMaxDepth)\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mforestSize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# // hóa với prange thay range\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mbootstrappedDataFrame\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbootstrapSample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataFrame\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbootstrapSize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0mdecisionTree\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuildDecisionTree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbootstrappedDataFrame\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxDepth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtreeMaxDepth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandomAttributes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandomAttributes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandomSplits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandomSplits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0mforest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecisionTree\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mforest\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-14-f344043ce1de>\u001b[0m in \u001b[0;36mbuildDecisionTree\u001b[0;34m(dataFrame, currentDepth, minSampleSize, maxDepth, randomAttributes, randomSplits)\u001b[0m\n\u001b[1;32m    156\u001b[0m         \u001b[0mcurrentDepth\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m         \u001b[0mpotentialSplits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetPotentialSplits\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandomAttributes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 158\u001b[0;31m         \u001b[0msplitColumn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplitValue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdetermineBestSplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpotentialSplits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandomSplits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    159\u001b[0m         \u001b[0mdataBelow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataAbove\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msplitData\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplitColumn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplitValue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataBelow\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataAbove\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-14-f344043ce1de>\u001b[0m in \u001b[0;36mdetermineBestSplit\u001b[0;34m(data, potentialSplits, randomSplits)\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'{randomSplitValue=}\\n{dataBelow=}\\n {dataAbove=}\\n\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m             \u001b[0mcurrentOverallEntropy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalculateOverallEntropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataBelow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataAbove\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    134\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mcurrentOverallEntropy\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0moverallEntropy\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m                 \u001b[0moverallEntropy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcurrentOverallEntropy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-14-f344043ce1de>\u001b[0m in \u001b[0;36mcalculateOverallEntropy\u001b[0;34m(dataBelow, dataAbove)\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[0mgridSizeBelow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataBelow\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mblockSize\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0mblockSize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m     \u001b[0mgridSizeAbove\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataAbove\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mblockSize\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0mblockSize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m     \u001b[0mcalculateEntropy\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mgridSizeBelow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblockSize\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataBelow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mentropyBelow\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m     \u001b[0mcalculateEntropy\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mgridSizeAbove\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblockSize\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataAbove\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mentropyAbove\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numba/cuda/dispatcher.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    489\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 491\u001b[0;31m         return self.dispatcher.call(args, self.griddim, self.blockdim,\n\u001b[0m\u001b[1;32m    492\u001b[0m                                     self.stream, self.sharedmem)\n\u001b[1;32m    493\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numba/cuda/dispatcher.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, args, griddim, blockdim, stream, sharedmem)\u001b[0m\n\u001b[1;32m    623\u001b[0m             \u001b[0mkernel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moverloads\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    624\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 625\u001b[0;31m             \u001b[0mkernel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_dispatcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDispatcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cuda_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    626\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    627\u001b[0m         \u001b[0mkernel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlaunch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgriddim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblockdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msharedmem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numba/cuda/dispatcher.py\u001b[0m in \u001b[0;36m_compile_for_args\u001b[0;34m(self, *args, **kws)\u001b[0m\n\u001b[1;32m    631\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mkws\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    632\u001b[0m         \u001b[0margtypes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtypeof_pyval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 633\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margtypes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    634\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    635\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtypeof_pyval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numba/cuda/dispatcher.py\u001b[0m in \u001b[0;36mcompile\u001b[0;34m(self, sig)\u001b[0m\n\u001b[1;32m    792\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Compilation disabled\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    793\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 794\u001b[0;31m             \u001b[0mkernel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Kernel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpy_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margtypes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtargetoptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    795\u001b[0m             \u001b[0;31m# We call bind to force codegen, so that there is a cubin to cache\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    796\u001b[0m             \u001b[0mkernel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numba/core/compiler_lock.py\u001b[0m in \u001b[0;36m_acquire_compile_lock\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0m_acquire_compile_lock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_acquire_compile_lock\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numba/cuda/dispatcher.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, py_func, argtypes, link, debug, lineinfo, inline, fastmath, extensions, max_registers, opt, device)\u001b[0m\n\u001b[1;32m     73\u001b[0m         }\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m         cres = compile_cuda(self.py_func, types.void, self.argtypes,\n\u001b[0m\u001b[1;32m     76\u001b[0m                             \u001b[0mdebug\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m                             \u001b[0mlineinfo\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlineinfo\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numba/core/compiler_lock.py\u001b[0m in \u001b[0;36m_acquire_compile_lock\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0m_acquire_compile_lock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_acquire_compile_lock\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numba/cuda/compiler.py\u001b[0m in \u001b[0;36mcompile_cuda\u001b[0;34m(pyfunc, return_type, args, debug, lineinfo, inline, fastmath, nvvm_options)\u001b[0m\n\u001b[1;32m    210\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mnumba\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_extension\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtarget_override\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtarget_override\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cuda'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 212\u001b[0;31m         cres = compiler.compile_extra(typingctx=typingctx,\n\u001b[0m\u001b[1;32m    213\u001b[0m                                       \u001b[0mtargetctx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtargetctx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m                                       \u001b[0mfunc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpyfunc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numba/core/compiler.py\u001b[0m in \u001b[0;36mcompile_extra\u001b[0;34m(typingctx, targetctx, func, args, return_type, flags, locals, library, pipeline_class)\u001b[0m\n\u001b[1;32m    714\u001b[0m     pipeline = pipeline_class(typingctx, targetctx, library,\n\u001b[1;32m    715\u001b[0m                               args, return_type, flags, locals)\n\u001b[0;32m--> 716\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile_extra\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    717\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    718\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numba/core/compiler.py\u001b[0m in \u001b[0;36mcompile_extra\u001b[0;34m(self, func)\u001b[0m\n\u001b[1;32m    450\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlifted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlifted_from\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 452\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compile_bytecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    453\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    454\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcompile_ir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc_ir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlifted\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlifted_from\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numba/core/compiler.py\u001b[0m in \u001b[0;36m_compile_bytecode\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    518\u001b[0m         \"\"\"\n\u001b[1;32m    519\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc_ir\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 520\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compile_core\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    522\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_compile_ir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numba/core/compiler.py\u001b[0m in \u001b[0;36m_compile_core\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    497\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfail_reason\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    498\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mis_final_pipeline\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 499\u001b[0;31m                         \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    500\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    501\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mCompilerError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"All available pipelines exhausted\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numba/core/compiler.py\u001b[0m in \u001b[0;36m_compile_core\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    484\u001b[0m                 \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    485\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 486\u001b[0;31m                     \u001b[0mpm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    487\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcr\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m                         \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numba/core/compiler_machinery.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m    366\u001b[0m                     \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpipeline_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpass_desc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    367\u001b[0m                 \u001b[0mpatched_exception\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_patch_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 368\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mpatched_exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    369\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    370\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdependency_analysis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numba/core/compiler_machinery.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m    354\u001b[0m                 \u001b[0mpass_inst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_pass_registry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpass_inst\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    355\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpass_inst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCompilerPass\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 356\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_runPass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpass_inst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    357\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mBaseException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Legacy pass in use\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numba/core/compiler_lock.py\u001b[0m in \u001b[0;36m_acquire_compile_lock\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0m_acquire_compile_lock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_acquire_compile_lock\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numba/core/compiler_machinery.py\u001b[0m in \u001b[0;36m_runPass\u001b[0;34m(self, index, pss, internal_state)\u001b[0m\n\u001b[1;32m    309\u001b[0m                 \u001b[0mmutated\u001b[0m \u001b[0;34m|=\u001b[0m \u001b[0mcheck\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_initialization\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minternal_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mSimpleTimer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpass_time\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 311\u001b[0;31m                 \u001b[0mmutated\u001b[0m \u001b[0;34m|=\u001b[0m \u001b[0mcheck\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_pass\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minternal_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mSimpleTimer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfinalize_time\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m                 \u001b[0mmutated\u001b[0m \u001b[0;34m|=\u001b[0m \u001b[0mcheck\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_finalizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minternal_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numba/core/compiler_machinery.py\u001b[0m in \u001b[0;36mcheck\u001b[0;34m(func, compiler_state)\u001b[0m\n\u001b[1;32m    271\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcheck\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompiler_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 273\u001b[0;31m             \u001b[0mmangled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompiler_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    274\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmangled\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m                 msg = (\"CompilerPass implementations should return True/False. \"\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numba/core/typed_passes.py\u001b[0m in \u001b[0;36mrun_pass\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m    103\u001b[0m                               % (state.func_id.func_name,)):\n\u001b[1;32m    104\u001b[0m             \u001b[0;31m# Type inference\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m             typemap, return_type, calltypes, errs = type_inference_stage(\n\u001b[0m\u001b[1;32m    106\u001b[0m                 \u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtypingctx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m                 \u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtargetctx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numba/core/typed_passes.py\u001b[0m in \u001b[0;36mtype_inference_stage\u001b[0;34m(typingctx, targetctx, interp, args, return_type, locals, raise_errors)\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0minfer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_constraint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0;31m# return errors in case of partial typing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m         \u001b[0merrs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minfer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpropagate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraise_errors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mraise_errors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m         \u001b[0mtypemap\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrestype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcalltypes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minfer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraise_errors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mraise_errors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numba/core/typeinfer.py\u001b[0m in \u001b[0;36mpropagate\u001b[0;34m(self, raise_errors)\u001b[0m\n\u001b[1;32m   1084\u001b[0m                                   if isinstance(e, ForceLiteralArg)]\n\u001b[1;32m   1085\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mforce_lit_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1086\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1087\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1088\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moperator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mor_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_lit_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypingError\u001b[0m: Failed in cuda mode pipeline (step: nopython frontend)\nNo implementation of function Function(<built-in function lt>) found for signature:\n \n >>> lt(UniTuple(int32 x 2), int64)\n \nThere are 14 candidate implementations:\n  - Of which 2 did not match due to:\n  Operator Overload in function 'lt': File: unknown: Line unknown.\n    With argument(s): '(UniTuple(int32 x 2), int64)':\n   No match for registered cases:\n    * (bool, bool) -> bool\n    * (int8, int8) -> bool\n    * (int16, int16) -> bool\n    * (int32, int32) -> bool\n    * (int64, int64) -> bool\n    * (uint8, uint8) -> bool\n    * (uint16, uint16) -> bool\n    * (uint32, uint32) -> bool\n    * (uint64, uint64) -> bool\n    * (float32, float32) -> bool\n    * (float64, float64) -> bool\n  - Of which 12 did not match due to:\n  Overload of function 'lt': File: <numerous>: Line N/A.\n    With argument(s): '(UniTuple(int32 x 2), int64)':\n   No match.\n\nDuring: typing of intrinsic-call at <ipython-input-14-f344043ce1de> (68)\n\nFile \"<ipython-input-14-f344043ce1de>\", line 68:\ndef calculateEntropy(data, result):\n    <source elided>\n\n    if idx < len(data):\n    ^\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy = calculateAccuracy(randomForestTestResults, df.iloc[:, -1]) * 100\n",
        "accuracy"
      ],
      "metadata": {
        "id": "v6Q6wdFdfuix"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a = np.array([[1, 2, 3],\n",
        "             [4,5, 6],\n",
        "             [7, 8, 9]])"
      ],
      "metadata": {
        "id": "QdStqX9yOhnq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a[:, -1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SHABcxpBOznh",
        "outputId": "9d721fb3-2cf7-460e-e373-bc8b717ee6bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([3, 6, 9])"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    }
  ]
}