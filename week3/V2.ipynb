{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "collapsed_sections": [
        "vvdh5EPl1cf2"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# PARALLELIZE RANDOM FOREST"
      ],
      "metadata": {
        "id": "7vpdloU5lbkm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hàm hỗ trợ"
      ],
      "metadata": {
        "id": "nrZgqWvz6eig"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import numba as nb\n",
        "import math\n",
        "import random\n",
        "import time\n",
        "from numba import prange, njit, jit, typed, types, cuda\n",
        "from numba.typed import List, Dict\n",
        "from numba.core import types\n",
        "\n",
        "# To ignore warinings\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "fftBSw4ufDXo"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "2rHXYOthXhry",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "496a316c-3554-431b-c564-94e9254efdb2"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Hàm hỗ trợ phân tách tập train - test\n",
        "\n",
        "def trainTestSplit(dataFrame, testSize):\n",
        "    if isinstance(testSize, float):\n",
        "        testSize = round(testSize * len(dataFrame))\n",
        "    indices = dataFrame.index.tolist()\n",
        "    testIndices = random.sample(population = indices, k = testSize)\n",
        "    dataFrameTest = dataFrame.loc[testIndices]\n",
        "    dataFrameTrain = dataFrame.drop(testIndices)\n",
        "    return dataFrameTrain, dataFrameTest\n",
        "\n",
        "# Hàm hỗ trợ lọc và đến các giá trị unique\n",
        "\n",
        "@nb.njit\n",
        "def numba_unique_with_counts(arr):\n",
        "\n",
        "    unique_values = []\n",
        "    value_counts = []\n",
        "\n",
        "    for value in range(len(arr)):\n",
        "        if arr[value] not in unique_values:\n",
        "            unique_values.append(arr[value])\n",
        "            value_counts.append(1)\n",
        "        else:\n",
        "            index = unique_values.index(arr[value])\n",
        "            value_counts[index] += 1\n",
        "\n",
        "    return np.array(unique_values), np.array(value_counts)"
      ],
      "metadata": {
        "id": "guIv9IUD6eOj"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a = np.array([4, 1, 13, 1])"
      ],
      "metadata": {
        "id": "v1nknXL4Dl3F"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Decision Tree\n"
      ],
      "metadata": {
        "id": "mn_Zumfi1VDw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Kiểm tra data có thuộc một label duy nhất\n",
        "'''\n",
        "# @nb.jit\n",
        "def checkPurity(data):\n",
        "    uniqueVals, _ = numba_unique_with_counts(data[:, -1])\n",
        "    if len(uniqueVals) == 1:\n",
        "        return True\n",
        "    else:\n",
        "        return False\n",
        "\n",
        "\n",
        "'''\n",
        "Xác định label phổ biến nhất còn trong data và trả về label đó\n",
        "\n",
        "'''\n",
        "# @nb.jit\n",
        "def classifyData(data):\n",
        "    uniqueClasses, uniqueClassesCounts = numba_unique_with_counts(data[:, -1])\n",
        "\n",
        "\n",
        "    return uniqueClasses[uniqueClassesCounts.argmax()]\n",
        "\n",
        "\n",
        "'''\n",
        "Xác định cột (thuộc tính) và các giá trị split tiềm năng của cột đó\n",
        "'''\n",
        "\n",
        "@cuda.jit(cache=True)\n",
        "def split_values_kernel(uniqueValues, splitValues):\n",
        "\n",
        "  idx = cuda.grid(1)\n",
        "  if idx < len(uniqueValues):\n",
        "    splitValues[idx] = (uniqueValues[idx] + uniqueValues[idx + 1]) / 2\n",
        "\n",
        "\n",
        "\n",
        "def getPotentialSplits(data):\n",
        "  columns = data.shape[1]\n",
        "\n",
        "\n",
        "  columnsIndices = np.arange(columns - 1)\n",
        "  potentialSplits = np.empty(columns - 1, dtype=object)\n",
        "\n",
        "  for col in range(len(columnsIndices)):\n",
        "    values = data[:, columnsIndices[col]]\n",
        "\n",
        "    # uniqueValues = np.unique(values)\n",
        "    uniqueValues, _ = numba_unique_with_counts(values)\n",
        "\n",
        "\n",
        "    if len(uniqueValues) == 1:\n",
        "      potentialSplits[columnsIndices[col]] = uniqueValues\n",
        "\n",
        "    else:\n",
        "      splitValues = np.empty(len(uniqueValues - 1))\n",
        "\n",
        "      cd_split_vals = cuda.to_device(splitValues)\n",
        "\n",
        "      bs = 64\n",
        "      gs = (len(uniqueValues) + bs - 1) // bs\n",
        "\n",
        "      split_values_kernel[gs, bs](uniqueValues, cd_split_vals)\n",
        "\n",
        "      potentialSplits[columnsIndices[col]] = cd_split_vals.copy_to_host()\n",
        "\n",
        "  return potentialSplits\n",
        "\n",
        "\n",
        "'''\n",
        "# Tách data thành 2 phần dựa vào splitValue\n",
        "'''\n",
        "@nb.njit(fastmath=True)\n",
        "def splitData(data, splitColumn, splitValue):\n",
        "    # Lấy data của cột thuộc tính đang đc xét để so sánh\n",
        "    splitColumnValues = data[:, splitColumn]\n",
        "\n",
        "    # Tách nguyên data (không phải chỉ mỗi data ở cột splitColumnValues) thành hai phần, một phần gồm các giá trị ở cột splitColumnValues <= splitValue và ngược lại\n",
        "    return data[splitColumnValues <= splitValue], data[splitColumnValues > splitValue]\n",
        "\n",
        "'''\n",
        "# Tính entropy của từng phần data\n",
        "'''\n",
        "\n",
        "# @cuda.jit(cache=True)\n",
        "# def my_probabilities(uniqueClassesCounts, probs, s):\n",
        "\n",
        "#     idx = cuda.grid(1)\n",
        "\n",
        "#     if idx < len(uniqueClassesCounts):\n",
        "#       cuda.atomic.add(s, 0, uniqueClassesCounts[idx])\n",
        "\n",
        "#       probs[idx] = uniqueClassesCounts[idx] / s[0]\n",
        "\n",
        "# @cuda.jit(cache=True)\n",
        "# def my_entropy(arr, etp):\n",
        "\n",
        "#   idx = cuda.grid(1)\n",
        "\n",
        "#   if idx < len(arr):\n",
        "#     cuda.atomic.add(etp, 0, arr[idx] * -math.log2(arr[idx]))\n",
        "\n",
        "\n",
        "# # @nb.njit(fastmath=True)\n",
        "# def calculateEntropy(data):\n",
        "#     # print(data)\n",
        "\n",
        "#     if len(data) == 0: return 0\n",
        "\n",
        "#     # Tính entropy của data dựa vào tỷ lệ labels mà data thuộc về\n",
        "#     _, uniqueClassesCounts = numba_unique_with_counts(data[:, -1])\n",
        "\n",
        "#     probabilities = np.zeros(len(uniqueClassesCounts))\n",
        "#     s = np.array([0.0])\n",
        "#     cd_s = cuda.to_device(s)\n",
        "\n",
        "\n",
        "#     cd_probs = cuda.to_device(probabilities)\n",
        "\n",
        "#     block_size = 128\n",
        "\n",
        "#     grid_size = (len(uniqueClassesCounts) + block_size - 1) // block_size\n",
        "\n",
        "#     my_probabilities[grid_size, block_size](uniqueClassesCounts, cd_probs, cd_s)\n",
        "\n",
        "#     probs = cd_probs.copy_to_host()\n",
        "\n",
        "\n",
        "#     # cuda for entropy\n",
        "#     etp = np.array([0.0])\n",
        "#     cd_etp = cuda.to_device(etp)\n",
        "\n",
        "#     block_size = 128\n",
        "\n",
        "#     # grid_size = np.ceil(len(probabilities) / block_size)\n",
        "#     grid_size = (len(probs) + block_size - 1) // block_size\n",
        "#     my_entropy[grid_size, block_size](probs, cd_etp)\n",
        "\n",
        "#     return cd_etp.copy_to_host()[0]\n",
        "\n",
        "@nb.njit(fastmath=True)\n",
        "def calculateEntropy(data):\n",
        "\n",
        "    # Tính entropy của data dựa vào tỷ lệ labels mà data thuộc về\n",
        "    _, uniqueClassesCounts = numba_unique_with_counts(data[:, -1])\n",
        "    # _, uniqueClassesCounts = np.unique(data[:, -1], return_counts=True)\n",
        "    probabilities = uniqueClassesCounts / uniqueClassesCounts.sum()\n",
        "\n",
        "\n",
        "    return sum(probabilities * -np.log2(probabilities))\n",
        "\n",
        "\n",
        "'''\n",
        "# Tính tổng entropy\n",
        "'''\n",
        "\n",
        "# @nb.njit(fastmath=True)\n",
        "def calculateOverallEntropy(dataBelow, dataAbove):\n",
        "\n",
        "    # Lấy tỷ lệ số lượng data ở nửa trên so với số lượng data\n",
        "    pDataBelow = len(dataBelow) / (len(dataBelow) + len(dataAbove))\n",
        "\n",
        "    # Lấy tỷ lệ số lượng data ở nửa dưới so với số lượng data\n",
        "    pDataAbove = len(dataAbove) / (len(dataBelow) + len(dataAbove))\n",
        "\n",
        "    return pDataBelow * calculateEntropy(np.array(dataBelow)) + pDataAbove * calculateEntropy(np.array(dataAbove))\n",
        "\n",
        "\n",
        "\n",
        "'''\n",
        "Xác định cột (thuộc tính) và giá trị tiềm năng duy nhất của cột đó (splitPoint có entropy nhỏ nhất)\n",
        "'''\n",
        "\n",
        "# @nb.jit(parallel=True, fastmath=True)\n",
        "# def determineBestSplit(data, potentialSplits):\n",
        "#     overallEntropy = 9999\n",
        "#     bestSplitColumn = 0\n",
        "#     bestSplitValue = 0\n",
        "\n",
        "#     # Xét từng key là thuộc tính trong potentialSplits\n",
        "#     for splitColumn in nb.prange(len(potentialSplits)):\n",
        "#         # Xét từng value của từng thuộc tính trong potentialSplits\n",
        "#         for splitValue in nb.prange(len(potentialSplits[splitColumn])):\n",
        "#             # Tách nguyên data của làm hai phần nhỏ hơn và lớn hơn so với value đang xét\n",
        "#             splitColumnValues = data[:, splitColumn]\n",
        "\n",
        "#             # Tách nguyên data (không phải chỉ mỗi data ở cột splitColumnValues) thành hai phần, một phần gồm các giá trị ở cột splitColumnValues <= splitValue và ngược lại\n",
        "#             dataBelow, dataAbove =  data[splitColumnValues <= splitValue], data[splitColumnValues > splitValue]\n",
        "#             # dataBelow, dataAbove = splitData(data, splitColumn, splitValue)\n",
        "\n",
        "#             # Xét Entropy của 2 phần data đó, nếu nhỏ nhất thì lấy\n",
        "#             currentOverallEntropy = calculateOverallEntropy(dataBelow, dataAbove)\n",
        "#             if currentOverallEntropy <= overallEntropy:\n",
        "#                 overallEntropy = currentOverallEntropy\n",
        "#                 bestSplitColumn = splitColumn\n",
        "#                 bestSplitValue = splitValue\n",
        "#     return bestSplitColumn, bestSplitValue\n",
        "\n",
        "\n",
        "@nb.jit(parallel=True, fastmath=True)\n",
        "def determineBestSplit(data, potentialSplits):\n",
        "    overallEntropy = 9999\n",
        "    bestSplitColumn = 0\n",
        "    bestSplitValue = 0\n",
        "\n",
        "    # Xét từng key là thuộc tính trong potentialSplits\n",
        "    for splitColumn in nb.prange(len(potentialSplits)):\n",
        "        # Xét từng value của từng thuộc tính trong potentialSplits\n",
        "        for splitValue in nb.prange(len(potentialSplits[splitColumn])):\n",
        "            # Tách nguyên data của làm hai phần nhỏ hơn và lớn hơn so với value đang xét\n",
        "            dataBelow, dataAbove = splitData(data, splitColumn, splitValue)\n",
        "            # Xét Entropy của 2 phần data đó, nếu nhỏ nhất thì lấy\n",
        "            currentOverallEntropy = calculateOverallEntropy(dataBelow, dataAbove)\n",
        "            if currentOverallEntropy <= overallEntropy:\n",
        "                overallEntropy = currentOverallEntropy\n",
        "                bestSplitColumn = splitColumn\n",
        "                bestSplitValue = splitValue\n",
        "    return bestSplitColumn, bestSplitValue\n",
        "\n",
        "\n",
        "'''\n",
        "Xây dựng cây\n",
        "'''\n",
        "# @nb.jit(cache=True)\n",
        "def buildDecisionTree(dataFrame, currentDepth = 0, minSampleSize = 2, maxDepth = 1000, COLUMN_HEADERS = None):\n",
        "    # Nếu mới bắt đầu khởi tạo cây\n",
        "    if currentDepth == 0:\n",
        "        # global COLUMN_HEADERS\n",
        "        COLUMN_HEADERS = dataFrame.columns.to_numpy()\n",
        "        data = dataFrame.to_numpy()\n",
        "    else:\n",
        "        data = dataFrame\n",
        "        COLUMN_HEADERS = COLUMN_HEADERS\n",
        "    # Xét nếu tất cả dữ liệu đều có label giống nhau || dữ liệu dữ liệu ít hơn số dữ liệu tối thiểu để xét cho một node || độ cao đã đạt tối đa\n",
        "    if checkPurity(data) or len(data) < minSampleSize or currentDepth == maxDepth:\n",
        "        # Trả về label cho node cuối cùng đó\n",
        "        return classifyData(data)\n",
        "\n",
        "    # Nếu không thì tiếp tục quá trình tạo cây\n",
        "    else:\n",
        "        currentDepth += 1\n",
        "        # Lấy ra potentialSplits là dictionaries gồm key là các thuộc tính, values là các giá trị potential của mỗi thuộc tính\n",
        "        potentialSplits = getPotentialSplits(data)\n",
        "\n",
        "\n",
        "        # Lấy ra cột thuộc tính và giá trị split của thuộc tính mà mang lại giá trị entropy nhỏ nhất\n",
        "        splitColumn, splitValue = determineBestSplit(data, potentialSplits)\n",
        "\n",
        "        # split data theo 2 giá trị vừa lấy đc\n",
        "        dataBelow, dataAbove = splitData(data, splitColumn, splitValue)\n",
        "\n",
        "        # Nếu tất cả data đều thuộc một label rồi thì lấy label đó cho node\n",
        "        if len(dataBelow) == 0 or len(dataAbove) == 0:\n",
        "            return classifyData(data)\n",
        "        else:\n",
        "            # Đặt giá trị splitPoint cho thuộc tính đó, phần data nhỏ hơn sẽ qua yes và ngược lại\n",
        "            question = str(COLUMN_HEADERS[splitColumn]) + \" <= \" + str(splitValue)\n",
        "            decisionSubTree = {question: []}\n",
        "            # Tiếp tục rẽ nhánh bằng đệ quy\n",
        "            yesAnswer = buildDecisionTree(dataBelow, currentDepth, minSampleSize, maxDepth, COLUMN_HEADERS)\n",
        "            noAnswer = buildDecisionTree(dataAbove, currentDepth, minSampleSize, maxDepth, COLUMN_HEADERS)\n",
        "            # Nếu 2 nhánh giống nhau thì chỉ rẽ 1 nhánh, khác nhau thì rẽ 2 nhánh\n",
        "            if yesAnswer == noAnswer:\n",
        "                decisionSubTree = yesAnswer\n",
        "            else:\n",
        "                decisionSubTree[question].append(yesAnswer)\n",
        "                decisionSubTree[question].append(noAnswer)\n",
        "            return decisionSubTree\n",
        "\n",
        "\n",
        "'''\n",
        "Dùng để xét label cho tập dữ liệu bằng cách đưa mỗi cây trong randomForest\n",
        "'''\n",
        "# @nb.jit\n",
        "def classifySample(sample, decisionTree):\n",
        "    # Nếu xét đến label của node lá cuối cùng thì trả về label đó\n",
        "    if not isinstance(decisionTree, dict):\n",
        "        return decisionTree\n",
        "    question = list(decisionTree.keys())[0]\n",
        "    attribute, value = question.split(\" <= \")\n",
        "    # So sánh với splitValue để mò đường đi tới node chứa label quyết định\n",
        "    if sample[attribute] <= float(value):\n",
        "        answer = decisionTree[question][0]\n",
        "    else:\n",
        "        answer = decisionTree[question][1]\n",
        "    return classifySample(sample, answer)\n",
        "\n",
        "\n",
        "'''\n",
        "Trả về label được dự đoán của tập data\n",
        "'''\n",
        "def decisionTreePredictions(dataFrame, decisionTree):\n",
        "    # Dự đoán từng label của từng dòng dữ liệu\n",
        "    predictions = dataFrame.apply(classifySample, axis = 1, args = (decisionTree,))\n",
        "    return predictions\n",
        "\n",
        "\n",
        "'''\n",
        "# So sánh độ chính xác giữa tập đang ktra và tập giá trị thật\n",
        "'''\n",
        "def calculateAccuracy(predictedResults, category):\n",
        "    resultCorrect = predictedResults == category\n",
        "    return resultCorrect.mean()"
      ],
      "metadata": {
        "id": "x9ZrleYU1QBa"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Random Forest\n"
      ],
      "metadata": {
        "id": "vvdh5EPl1cf2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Dùng kỹ thuật boostrap để lấy random một subset của data\n",
        "'''\n",
        "# @nb.jit\n",
        "def bootstrapSample(dataFrame, bootstrapSize):\n",
        "    randomIndices = np.random.randint(low = 0, high = len(dataFrame), size = bootstrapSize)\n",
        "    return dataFrame.iloc[randomIndices]\n",
        "\n",
        "\n",
        "'''\n",
        "Xây dựng rừng cây từ các cây decision tree\n",
        "'''\n",
        "@nb.jit(parallel=True)\n",
        "def createRandomForest(dataFrame, bootstrapSize, forestSize = 20, treeMaxDepth = 1000):\n",
        "    forest = []\n",
        "    total_time=0\n",
        "    for i in nb.prange(forestSize):\n",
        "        # Lấy random một subset của data bằng kỹ thuật boostrap\n",
        "        bootstrappedDataFrame = bootstrapSample(dataFrame, bootstrapSize)\n",
        "        # Xây dựng 1 cây quyết định từ subset đó\n",
        "        start_time = time.time()\n",
        "        decisionTree = buildDecisionTree(bootstrappedDataFrame, maxDepth = treeMaxDepth, )\n",
        "        end_time = time.time()\n",
        "        total_time += (end_time - start_time)\n",
        "        # Thêm cây vào rừng\n",
        "        forest.append(decisionTree)\n",
        "    print(\"AVG time to build a Decision Tree =\",total_time/forestSize)\n",
        "    return forest\n",
        "\n",
        "\n",
        "'''\n",
        "Dự đoán data bằng cách đưa qua từng cây trong randomForest rồi lấy label theo số đông\n",
        "'''\n",
        "def randomForestPredictions(dataFrame, randomForest):\n",
        "    predictions = {}\n",
        "    # Đưa dữ liệu qua rừng cây (cụ thể là từng cây)\n",
        "    for i in range(len(randomForest)):\n",
        "\n",
        "        column = \"decision tree \" + str(i)\n",
        "        predictions[column] = decisionTreePredictions(dataFrame, randomForest[i])\n",
        "    predictions = pd.DataFrame(predictions)\n",
        "    # Lấy label có số lần xuất hiện nhiều nhất của từng dòng\n",
        "    return predictions.mode(axis = 1)[0]\n",
        "\n",
        "'''\n",
        "# So sánh độ chính xác giữa tập đang ktra và tập giá trị thật\n",
        "'''\n",
        "def calculateAccuracy(predictedResults, category):\n",
        "    # So sánh label của tập dự đoán và tập real\n",
        "    resultCorrect = predictedResults == category\n",
        "    return resultCorrect.mean()"
      ],
      "metadata": {
        "id": "F2O5OGmr1ebm"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Run with car_evaluation dataset"
      ],
      "metadata": {
        "id": "MFSU5qttnOVf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataFrame = pd.read_csv(\"/content/drive/MyDrive/Bài Tập Về Nhà Môn Cuối/random-forest-from-scratch-main/dataset_files/car_evaluation.csv\")\n",
        "\n",
        "buyingMapping = {\"low\": 1, \"med\": 2, \"high\": 3, \"vhigh\": 4}\n",
        "dataFrame[\"buying\"] = dataFrame[\"buying\"].map(buyingMapping)\n",
        "\n",
        "maintMapping = {\"low\": 1, \"med\": 2, \"high\": 3, \"vhigh\": 4}\n",
        "dataFrame[\"maint\"] = dataFrame[\"maint\"].map(maintMapping)\n",
        "\n",
        "doorsMapping = {\"2\": 2, \"3\": 3, \"4\": 4, \"5more\": 5}\n",
        "dataFrame[\"doors\"] = dataFrame[\"doors\"].map(doorsMapping)\n",
        "\n",
        "personsMapping = {\"2\": 2, \"4\": 4, \"more\": 6}\n",
        "dataFrame[\"persons\"] = dataFrame[\"persons\"].map(personsMapping)\n",
        "\n",
        "lugBootMapping = {\"small\": 1, \"med\": 2, \"big\": 3}\n",
        "dataFrame[\"lug_boot\"] = dataFrame[\"lug_boot\"].map(lugBootMapping)\n",
        "\n",
        "safetyMapping = {\"low\": 1, \"med\": 2, \"high\": 3}\n",
        "dataFrame[\"safety\"] = dataFrame[\"safety\"].map(safetyMapping)\n",
        "\n",
        "# Import label encoder\n",
        "from sklearn import preprocessing\n",
        "\n",
        "# label_encoder object knows\n",
        "# how to understand word labels.\n",
        "label_encoder = preprocessing.LabelEncoder()\n",
        "\n",
        "# Encode labels in column 'species'.\n",
        "dataFrame['class']= label_encoder.fit_transform(dataFrame['class'])\n",
        "\n",
        "dataFrameTrain, dataFrameTest = trainTestSplit(dataFrame, testSize = 0.3)\n",
        "\n",
        "print(\"Random Forest - Car Evaluation Dataset\")\n",
        "print(\"  Maximum bootstrap size (n) is {}\".format(dataFrameTrain.shape[0]))\n",
        "print(\"  Maximum random subspace size (d) is {}\".format(dataFrameTrain.shape[1] - 1))"
      ],
      "metadata": {
        "id": "lXEhUEvoncmq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9fbf2e8f-2dcd-4f83-e678-6fbf1e9cb91b"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Forest - Car Evaluation Dataset\n",
            "  Maximum bootstrap size (n) is 1210\n",
            "  Maximum random subspace size (d) is 6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### First time run"
      ],
      "metadata": {
        "id": "WyGQCdFD6U5j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "startTime = time.time()\n",
        "randomForest = createRandomForest(dataFrameTrain, bootstrapSize = 1000, forestSize = 10, treeMaxDepth = 8)\n",
        "buildingTime = time.time() - startTime\n",
        "randomForestTestResults = randomForestPredictions(dataFrameTest, randomForest)\n",
        "accuracyTest = calculateAccuracy(randomForestTestResults, dataFrameTest.iloc[:, -1]) * 100\n",
        "randomForestTrainResults = randomForestPredictions(dataFrameTrain, randomForest)\n",
        "accuracyTrain = calculateAccuracy(randomForestTrainResults, dataFrameTrain.iloc[:, -1]) * 100\n",
        "print(\"  n = {}, k = {}, maxDepth = {}:\".format(1000, 10, 8))\n",
        "print(\"    accTest = {0:.2f}%, \".format(accuracyTest), end = \"\")\n",
        "print(\"accTrain = {0:.2f}%, \".format(accuracyTrain), end = \"\")\n",
        "print(\"buildTime = {0:.2f}s\".format(buildingTime), end = \"\\n\")\n",
        "\n",
        "num_threads = nb.get_num_threads()\n",
        "print(\"\\n\\nNumber of threads used by Numba:\", num_threads)\n",
        "\n",
        "import numba.cuda as cuda\n",
        "\n",
        "if cuda.is_available():\n",
        "    print(\"CUDA is available.\")\n",
        "    current_device = cuda.get_current_device()\n",
        "    print(f\"Using CUDA device: {current_device.name}\")\n",
        "else:\n",
        "    print(\"CUDA is not available. Running on CPU.\")"
      ],
      "metadata": {
        "id": "F1buQncRpDBw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "32f7a909-71db-4edc-d653-0bd4f17fdf03"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AVG time to build a Decision Tree = 0.5583487749099731\n",
            "  n = 1000, k = 10, maxDepth = 8:\n",
            "    accTest = 86.68%, accTrain = 88.26%, buildTime = 6.65s\n",
            "\n",
            "\n",
            "Number of threads used by Numba: 2\n",
            "CUDA is available.\n",
            "Using CUDA device: b'Tesla T4'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Second time run"
      ],
      "metadata": {
        "id": "lUGTV3MJ6XD6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "startTime = time.time()\n",
        "randomForest = createRandomForest(dataFrameTrain, bootstrapSize = 1000, forestSize = 10, treeMaxDepth = 8)\n",
        "buildingTime = time.time() - startTime\n",
        "randomForestTestResults = randomForestPredictions(dataFrameTest, randomForest)\n",
        "accuracyTest = calculateAccuracy(randomForestTestResults, dataFrameTest.iloc[:, -1]) * 100\n",
        "randomForestTrainResults = randomForestPredictions(dataFrameTrain, randomForest)\n",
        "accuracyTrain = calculateAccuracy(randomForestTrainResults, dataFrameTrain.iloc[:, -1]) * 100\n",
        "print(\"  n = {}, k = {}, maxDepth = {}:\".format(1000, 10, 8))\n",
        "print(\"    accTest = {0:.2f}%, \".format(accuracyTest), end = \"\")\n",
        "print(\"accTrain = {0:.2f}%, \".format(accuracyTrain), end = \"\")\n",
        "print(\"buildTime = {0:.2f}s\".format(buildingTime), end = \"\\n\")\n",
        "\n",
        "num_threads = nb.get_num_threads()\n",
        "print(\"\\n\\nNumber of threads used by Numba:\", num_threads)\n",
        "\n",
        "import numba.cuda as cuda\n",
        "\n",
        "if cuda.is_available():\n",
        "    print(\"CUDA is available.\")\n",
        "    current_device = cuda.get_current_device()\n",
        "    print(f\"Using CUDA device: {current_device.name}\")\n",
        "else:\n",
        "    print(\"CUDA is not available. Running on CPU.\")"
      ],
      "metadata": {
        "id": "x8LLx_T46aIJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9a00000a-adad-45d9-c5f6-231b188f3bd8"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AVG time to build a Decision Tree = 0.21154999732971191\n",
            "  n = 1000, k = 10, maxDepth = 8:\n",
            "    accTest = 86.49%, accTrain = 89.50%, buildTime = 2.12s\n",
            "\n",
            "\n",
            "Number of threads used by Numba: 2\n",
            "CUDA is available.\n",
            "Using CUDA device: b'Tesla T4'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Run with breastCancer dataset"
      ],
      "metadata": {
        "id": "fn5KZ9CMpcdO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataFrame = pd.read_csv(\"/content/drive/MyDrive/Bài Tập Về Nhà Môn Cuối/random-forest-from-scratch-main/dataset_files/breast_cancer.csv\")\n",
        "\n",
        "# Import label encoder\n",
        "from sklearn import preprocessing\n",
        "\n",
        "# label_encoder object knows\n",
        "# how to understand word labels.\n",
        "label_encoder = preprocessing.LabelEncoder()\n",
        "\n",
        "# Encode labels in column 'species'.\n",
        "dataFrame['diagnosis']= label_encoder.fit_transform(dataFrame['diagnosis'])\n",
        "\n",
        "dataFrame = dataFrame.drop(\"id\", axis = 1)\n",
        "dataFrame = dataFrame[dataFrame.columns.tolist()[1: ] + dataFrame.columns.tolist()[0: 1]]\n",
        "\n",
        "dataFrameTrain, dataFrameTest = trainTestSplit(dataFrame, testSize = 0.25)\n",
        "\n",
        "print(\"Random Forest - Breast Cancer Dataset\")\n",
        "print(\"  Maximum bootstrap size (n) is {}\".format(dataFrameTrain.shape[0]))\n",
        "print(\"  Maximum random subspace size (d) is {}\".format(dataFrameTrain.shape[1] - 1))"
      ],
      "metadata": {
        "id": "NyTiCrX0paf5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c25d70bf-79de-4602-b78c-8973ddd4802b"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Forest - Breast Cancer Dataset\n",
            "  Maximum bootstrap size (n) is 427\n",
            "  Maximum random subspace size (d) is 30\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### First time run"
      ],
      "metadata": {
        "id": "fkopn6ru6Fa-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "startTime = time.time()\n",
        "randomForest = createRandomForest(dataFrameTrain, bootstrapSize = 60, forestSize = 30, treeMaxDepth = 3)\n",
        "buildingTime = time.time() - startTime\n",
        "randomForestTestResults = randomForestPredictions(dataFrameTest, randomForest)\n",
        "accuracyTest = calculateAccuracy(randomForestTestResults, dataFrameTest.iloc[:, -1]) * 100\n",
        "randomForestTrainResults = randomForestPredictions(dataFrameTrain, randomForest)\n",
        "accuracyTrain = calculateAccuracy(randomForestTrainResults, dataFrameTrain.iloc[:, -1]) * 100\n",
        "print(\"  n = {}, k = {}, maxDepth = {}:\".format(60, 30, 3))\n",
        "print(\"    accTest = {0:.2f}%, \".format(accuracyTest), end = \"\")\n",
        "print(\"accTrain = {0:.2f}%, \".format(accuracyTrain), end = \"\")\n",
        "print(\"buildTime = {0:.2f}s\".format(buildingTime), end = \"\\n\")"
      ],
      "metadata": {
        "id": "_Gqc2jyIqFkw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bf64ce0a-2844-4fa2-eb3e-6d87baa4d2e1"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AVG time to build a Decision Tree = 0.31408650875091554\n",
            "  n = 60, k = 30, maxDepth = 3:\n",
            "    accTest = 92.25%, accTrain = 93.21%, buildTime = 9.44s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Second time run"
      ],
      "metadata": {
        "id": "tFmbetwC6Lv3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "startTime = time.time()\n",
        "randomForest = createRandomForest(dataFrameTrain, bootstrapSize = 60, forestSize = 30, treeMaxDepth = 3)\n",
        "buildingTime = time.time() - startTime\n",
        "randomForestTestResults = randomForestPredictions(dataFrameTest, randomForest)\n",
        "accuracyTest = calculateAccuracy(randomForestTestResults, dataFrameTest.iloc[:, -1]) * 100\n",
        "randomForestTrainResults = randomForestPredictions(dataFrameTrain, randomForest)\n",
        "accuracyTrain = calculateAccuracy(randomForestTrainResults, dataFrameTrain.iloc[:, -1]) * 100\n",
        "print(\"  n = {}, k = {}, maxDepth = {}:\".format(60, 30, 3))\n",
        "print(\"    accTest = {0:.2f}%, \".format(accuracyTest), end = \"\")\n",
        "print(\"accTrain = {0:.2f}%, \".format(accuracyTrain), end = \"\")\n",
        "print(\"buildTime = {0:.2f}s\".format(buildingTime), end = \"\\n\")"
      ],
      "metadata": {
        "id": "mqJl3RqR6N-W",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "76adb71a-5d42-423f-baea-92649ad610a2"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AVG time to build a Decision Tree = 0.14782694180806477\n",
            "  n = 60, k = 30, maxDepth = 3:\n",
            "    accTest = 92.25%, accTrain = 93.44%, buildTime = 4.45s\n"
          ]
        }
      ]
    }
  ]
}